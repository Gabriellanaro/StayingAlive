{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8681ab4e",
   "metadata": {},
   "source": [
    "# Stayin' Alive\n",
    "### *An AI-Powered Tool for Optimal Restaurant and Bar Location Selection and Business Longevity*\n",
    "42578 – Advanced Business Analytics, DTU, 2025 <br>\n",
    "Group 21 - Crocs Validation<br>\n",
    "Giulia Andreatta -sxxxx<br>\n",
    "Gabriel Lanaro - sxxxx<br>\n",
    "Alessia Saccardo - sxxxx<br>\n",
    "Gabriele Turetta - sxxxx<br>\n",
    "\n",
    "### Objective\n",
    "Opening a restaurant or a bar is a high-risk endeavor—many establishments close within their first few years. In Copenhagen, aspiring restaurateurs and investors often lack a data-driven approach when selecting a location. Moreover, understanding the reasons behind a restaurant’s success or failure remains a challenge.\n",
    "\n",
    "This project aims to:\n",
    "\n",
    "- Recommend optimal locations for new restaurants or bars using Survival Analysis.\n",
    "- Visualize location suitability through an interactive heatmap enriched with predictive longevity scores, pedestrian peak hours, density of restaurants, pins of active and closed activities.\n",
    "\n",
    "### Datasets\n",
    "\n",
    "- **Company data scraped from the official CVR registry via [virk.dk](https://datacvr.virk.dk/soegeresultater?fritekst=d&sideIndex=0&size=10)**<br>\n",
    "Includes business registration details, location, restaurant closures, branchekode\n",
    "\n",
    "- **Google Maps Scraped Data**<br>\n",
    "Includes business location, rating, number of reviews, price range, tags.\n",
    "\n",
    "- **Pedestrian Dataset from [OpenData.dk](https://www.opendata.dk/city-of-copenhagen/taelling_fodg#:~:text=Number%20of%20pedestrians%20counted%20on,19%20in%20both%20directions)**<br>\n",
    "Provides foot traffic counts recorded at specific times and locations in Copenhagen\n",
    "\n",
    "### ABA Topics Covered\n",
    "- **Web Data Mining**\n",
    "Scraping large-scale data from Google Maps and government databases to construct the datasets.\n",
    "\n",
    "- **Survival Analysis**\n",
    "Predicting restaurant longevity using Kaplan-Meier and Cox Proportional Hazards models.\n",
    "\n",
    "- **Recommender Systems**\n",
    "Suggesting location options for new restaurants and bars based on market gaps and existing competition.\n",
    "\n",
    "- **AI in the Real World**\n",
    "Delivering real value to stakeholders by supporting data-driven restaurant planning and resilience strategies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bab528",
   "metadata": {},
   "source": [
    "## 1st Step - Data Scraping from the official CVR registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41cae2e",
   "metadata": {},
   "source": [
    "This script performs web scraping on the Danish company registry website (https://datacvr.virk.dk)\n",
    "to extract company details for active business units in specified industry sectors (branchekoder).\n",
    "It uses Selenium to navigate the search results, extract key information for each business unit,\n",
    "and follow links to detailed company pages to obtain start and end dates.\n",
    "\n",
    "The results are saved in a CSV file, and duplicate entries (based on P-number) are avoided by \n",
    "keeping track of already seen values. The script is designed to be resumed without duplicating \n",
    "previous entries.\n",
    "\n",
    "Required: chromedriver installed and path correctly set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "driver_path = r\"C:\\\\programmi\\\\chromedriver\\\\chromedriver.exe\"\n",
    "options = Options()\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(driver_path), options=options)\n",
    "\n",
    "\n",
    "# the loop can be run for each branchekode separately, or all at once by uncommenting the lines below.\n",
    "# In our project, the branchekodes were manually uncommented, to create a single csv file for each branchekode.\n",
    "# This was done to avoid the script from running for too long, risking on IP bans or website crashes.\n",
    "# The single csv files were then merged into a single csv file.\n",
    "branchekodes = [\n",
    "    # 561110,   # serving food in restaurants and cafes\n",
    "    # 561190,   # includes the operation of restaurants, where the main emphasis is on takeaway with very limited table service.\n",
    "    # 563010,   # includes serving beverages, possibly with some edibles, but where the main emphasis is on serving non-alcoholic beverages for immediate consumption on site.\n",
    "    563020,     #  includes serving beverages, possibly with some edibles, but where the main emphasis is on serving alcoholic beverages for immediate consumption on the premises.\n",
    "]\n",
    "\n",
    "# Scraping Structure overview:\n",
    "# - Main search results are loaded via URL with parameters: sideIndex (pagination), branchekode (industry), etc.\n",
    "# - Each company entry is a 'div.row' within a 'div[data-cy=\"soegeresultater-tabel\"]'.\n",
    "# - Basic info (name, address, P-nummer, status, company type) is extracted directly from the search results.\n",
    "# - For each company, the script follows the \"Show More\" link in a new browser tab to extract Start date (Startdato)\n",
    "#   and End date (Ophørsdato), which appear in divs following label tags (either <strong> or <span>).\n",
    "for branchekode in branchekodes:\n",
    "    page = 0\n",
    "    csv_file_path = f\"scraped_companies_{branchekode}.csv\"\n",
    "    header = [\"Name\", \"Address\", \"P-nummer\", \"Status\", \"Company Type\", \"Startdate\", \"Enddate\"]\n",
    "    pnummer_seen = set()\n",
    "\n",
    "    # Load existing P-numbers if the file already exists to avoid duplicates\n",
    "    file_exists = os.path.exists(csv_file_path)\n",
    "    if file_exists:\n",
    "        with open(csv_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                pnummer_seen.add(row[\"P-nummer\"])\n",
    "    else:\n",
    "        # Create file and write header\n",
    "        with open(csv_file_path, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=header)\n",
    "            writer.writeheader()\n",
    "\n",
    "    while True:\n",
    "        url = f\"https://datacvr.virk.dk/soegeresultater?sideIndex={page}&enhedstype=produktionsenhed&region=29190623&branchekode={branchekode}\"\n",
    "        print(f\"Scraping page {page}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Get all company rows from the result table\n",
    "        rows = driver.find_elements(By.CSS_SELECTOR, 'div[data-cy=\"soegeresultater-tabel\"] > div.row')\n",
    "\n",
    "        if not rows:\n",
    "            print(\"No data found. Stopping.\")\n",
    "            break\n",
    "\n",
    "        # Process each company in the current page\n",
    "        for row in rows:\n",
    "            try:\n",
    "                name = row.find_element(By.CSS_SELECTOR, \"span.bold.value\").text.strip()\n",
    "\n",
    "                address_block = row.find_element(By.CSS_SELECTOR, \"div.col-12.col-lg-4\")\n",
    "                address_lines = address_block.text.strip().split(\"\\n\")[-2:]\n",
    "                address = \", \".join(address_lines)\n",
    "\n",
    "                pnummer = row.find_element(By.XPATH, './/div[div[text()=\"P-nummer:\"]]/div[2]').text.strip()\n",
    "\n",
    "                # Skip if already saved\n",
    "                if pnummer in pnummer_seen:\n",
    "                    continue\n",
    "                pnummer_seen.add(pnummer)\n",
    "\n",
    "                status = row.find_element(By.XPATH, './/div[div[text()=\"Status:\"]]/div[2]').text.strip()\n",
    "                form = row.find_element(By.XPATH, './/div[div[text()=\"Virksomhedsform:\"]]/div[2]').text.strip()\n",
    "\n",
    "                link_elem = row.find_element(By.CSS_SELECTOR, 'div[data-cy=\"vis-mere\"] a')\n",
    "                link = link_elem.get_attribute(\"href\")\n",
    "\n",
    "                # Open detail page in new tab\n",
    "                driver.execute_script(\"window.open('');\")\n",
    "                driver.switch_to.window(driver.window_handles[1])\n",
    "                driver.get(link)\n",
    "                time.sleep(3)\n",
    "\n",
    "                # Extract dates\n",
    "                startdato = \"\"\n",
    "                ophoersdato = \"\"\n",
    "\n",
    "                # Extract start and end dates from the detail page\n",
    "                try:\n",
    "                    startdato_element = driver.find_element(\n",
    "                        By.XPATH, '//div[(strong[text()=\"Startdato\"] or span[text()=\"Startdato\"])]/following-sibling::div'\n",
    "                    )\n",
    "                    startdato = startdato_element.text.strip()\n",
    "                except:\n",
    "                    startdato = \"\"\n",
    "\n",
    "                try:\n",
    "                    ophoersdato_element = driver.find_element(\n",
    "                        By.XPATH, '//div[(strong[text()=\"Ophørsdato\"] or span[text()=\"Ophørsdato\"])]/following-sibling::div'\n",
    "                    )\n",
    "                    ophoersdato = ophoersdato_element.text.strip()\n",
    "                except:\n",
    "                    ophoersdato = \"\"\n",
    "\n",
    "                # Close the detail tab and return to the main results tab\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "                # Write to CSV\n",
    "                with open(csv_file_path, \"a\", newline='', encoding=\"utf-8\") as f:\n",
    "                    writer = csv.DictWriter(f, fieldnames=header)\n",
    "                    writer.writerow({\n",
    "                        \"Name\": name,\n",
    "                        \"Address\": address,\n",
    "                        \"P-nummer\": pnummer,\n",
    "                        \"Status\": status,\n",
    "                        \"Company Type\": form,\n",
    "                        \"Startdate\": startdato,\n",
    "                        \"Enddate\": ophoersdato\n",
    "                    })\n",
    "\n",
    "                print(f\"{name} | {startdato} → {ophoersdato}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Error during parsing:\", e)\n",
    "                continue\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "driver.quit()\n",
    "print(\"Scraping finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81ae89",
   "metadata": {},
   "source": [
    "## 2nd Step - Geocoding Restaurant Addresses Using OpenStreetMap API\n",
    "This script performs address geocoding using the OpenStreetMap (OSM) API via the geopy library. It takes as input a CSV file containing restaurant records with address fields but missing geographic coordinates. For each address, it attempts to retrieve the corresponding latitude and longitude, which are then saved in a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ec83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "input_file = \"scraped_companies_combined_clean.csv\"\n",
    "output_file = \"scraped_companies_combined_clean_with_coords.csv\"\n",
    "\n",
    "# === LOAD ORIGINAL DATA ===\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Function to simplify the address before geocoding\n",
    "def simplify_address(row):\n",
    "    addr = str(row[\"Address\"])\n",
    "    addr = addr.split(\",\")[0].strip()  # only keep the part before the first comma\n",
    "    return f\"{addr}, Denmark\"\n",
    "\n",
    "# Add coordinate columns if they don't exist\n",
    "if \"latitude\" not in df.columns:\n",
    "    df[\"latitude\"] = None\n",
    "if \"longitude\" not in df.columns:\n",
    "    df[\"longitude\"] = None\n",
    "\n",
    "# Load already geocoded addresses to avoid duplicates\n",
    "already_done = set()\n",
    "if os.path.exists(output_file):\n",
    "    df_existing = pd.read_csv(output_file)\n",
    "    already_done = set(df_existing[\"Address\"].dropna().unique())\n",
    "    print(f\"Resuming from {len(already_done)} already completed addresses.\")\n",
    "\n",
    "# Filter rows that still need geocoding\n",
    "df_to_process = df[~df[\"Address\"].isin(already_done)].copy()\n",
    "print(f\"Addresses to geocode: {len(df_to_process)}\")\n",
    "\n",
    "# Initialize OpenStreetMap geocoder with delay to respect rate limits\n",
    "geolocator = Nominatim(user_agent=\"stayin_alive_simple_geocoder\")\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1.5)\n",
    "\n",
    "# Progressive saving to CSV (append mode!!)\n",
    "with open(output_file, \"a\", encoding=\"utf-8\", newline=\"\") as f_out:\n",
    "    header_written = os.stat(output_file).st_size == 0\n",
    "    for i, row in df_to_process.iterrows():\n",
    "        full_address = simplify_address(row)\n",
    "        try:\n",
    "            location = geocode(full_address)\n",
    "            if location:\n",
    "                row[\"latitude\"] = location.latitude\n",
    "                row[\"longitude\"] = location.longitude\n",
    "                print(f\"{full_address} -> ({location.latitude}, {location.longitude})\")\n",
    "            else:\n",
    "                print(f\"{full_address} -> not found\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error on {full_address}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Append row to output CSV\n",
    "        pd.DataFrame([row]).to_csv(f_out, index=False, header=header_written)\n",
    "        header_written = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d13553",
   "metadata": {},
   "source": [
    "## 3rd Step - Scraping Restaurant Metadata from Google Maps with Selenium\n",
    "This script performs web scraping from Google Maps using Selenium to enrich the dataset of restaurants obtained from steps 1-2. For each restaurant entry (name and address), the script:\n",
    "\n",
    "1. Opens a Google Maps search page\n",
    "\n",
    "2. Extracts the official listing title, star rating, number of reviews, price level, and associated category tags if present\n",
    "\n",
    "3. Saves the collected data into a CSV file\n",
    "\n",
    "4. The script supports resuming interrupted sessions by skipping entries that have already been saved to the output CSV file.\n",
    "\n",
    "Required: chromedriver installed and path correctly set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "driver_path = r\"C:\\Program Files\\chromedriver\\chromedriver.exe\"\n",
    "options = Options()\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "# options.add_argument(\"--headless\")  # Uncomment to run without opening browser window\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(driver_path), options=options)\n",
    "\n",
    "# === INPUT & OUTPUT PATHS ===\n",
    "csv_input_path = r\"C:\\Users\\Admin\\Documents\\HCAI\\ADVANCED_BUSINESS_ANALYTICS\\StayingAlive\\StayingAlive\\src\\scraping_correct\\scraped_companies_563020_notactive.csv\"\n",
    "csv_output_path = r\"C:\\Users\\Admin\\Documents\\HCAI\\ADVANCED_BUSINESS_ANALYTICS\\StayingAlive\\StayingAlive\\src\\scraping_correct\\maps_data_scraped.csv\"\n",
    "\n",
    "# Load input data\n",
    "df_input = pd.read_csv(csv_input_path)\n",
    "restaurant_data = df_input.to_dict(orient=\"records\")\n",
    "\n",
    "# Load already saved entries (if output file exists)\n",
    "saved_entries = set()\n",
    "header = [\"Input Name\", \"Input Address\", \"Title\", \"Rating\", \"Reviews\", \"Price Level\", \"Tags\"]\n",
    "\n",
    "if os.path.exists(csv_output_path):\n",
    "    with open(csv_output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            key = (row[\"Input Name\"], row[\"Input Address\"])\n",
    "            saved_entries.add(key)\n",
    "else:\n",
    "    with open(csv_output_path, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "\n",
    "# === MAIN LOOP OVER RESTAURANTS ===\n",
    "for entry in restaurant_data:\n",
    "    name = entry[\"Name\"]\n",
    "    address = entry[\"Address\"]\n",
    "    key = (name, address)\n",
    "\n",
    "    if key in saved_entries:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"Searching: {name} @ {address}\")\n",
    "        query = f\"{name} {address}\".replace(\" \", \"+\")\n",
    "        linkmaps = f\"https://www.google.com/maps/search/{query}\"\n",
    "        print(f\"URL: {linkmaps}\")\n",
    "        driver.get(linkmaps)\n",
    "        time.sleep(2)\n",
    "\n",
    "        try:\n",
    "            title = driver.find_element(By.CSS_SELECTOR, 'h1.DUwDvf').text\n",
    "        except:\n",
    "            title = \"\"\n",
    "\n",
    "        try:\n",
    "            rating = driver.find_element(By.CSS_SELECTOR, 'div.F7nice > span span[aria-hidden=\"true\"]').text\n",
    "        except:\n",
    "            rating = \"\"\n",
    "\n",
    "        try:\n",
    "            reviews_elem = driver.find_element(By.CSS_SELECTOR, 'div.F7nice > span span[aria-label$=\"reviews\"]').text\n",
    "            reviews = reviews_elem.strip(\"()\")\n",
    "        except:\n",
    "            reviews = \"\"\n",
    "\n",
    "        try:\n",
    "            price_level = driver.find_element(By.CSS_SELECTOR, 'div.DfOCNb.fontBodyMedium > div').text.split('\\n')[0]\n",
    "        except:\n",
    "            price_level = \"\"\n",
    "\n",
    "        try:\n",
    "            outer_divs = driver.find_elements(By.CSS_SELECTOR, \"div.KNfEk.aUjao\")\n",
    "            tags = []\n",
    "            for div in outer_divs:\n",
    "                try:\n",
    "                    tag = div.find_element(By.CSS_SELECTOR, \"div.tXNTee span.uEubGf.fontBodyMedium\").text\n",
    "                    tags.append(tag)\n",
    "                except:\n",
    "                    continue\n",
    "            tags = \", \".join(tags)\n",
    "        except:\n",
    "            tags = \"\"\n",
    "\n",
    "        # Append to CSV immediately\n",
    "        with open(csv_output_path, \"a\", newline='', encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=header)\n",
    "            writer.writerow({\n",
    "                \"Input Name\": name,\n",
    "                \"Input Address\": address,\n",
    "                \"Title\": title,\n",
    "                \"Rating\": rating,\n",
    "                \"Reviews\": reviews,\n",
    "                \"Price Level\": price_level,\n",
    "                \"Tags\": tags\n",
    "            })\n",
    "\n",
    "        print(f\"Saved: {title}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during scraping:\", e)\n",
    "        continue\n",
    "\n",
    "driver.quit()\n",
    "print(\"Scraping finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f50eb",
   "metadata": {},
   "source": [
    "## 4th Step - 1st Heatmap, an interactive Spatial Visualization of Restaurants/Bars and Pedestrian Traffic in Copenhagen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5985d62b",
   "metadata": {},
   "source": [
    "This script generates an interactive Folium heatmap that visualizes restaurant/bar locations and pedestrian traffic in Copenhagen. It combines multiple layers of spatial data to support exploratory analysis for business location decisions. \n",
    "This initial heatmap with multiple layers serves as a visual foundation for later overlaying the survival analysis scores as a new layer in the heatmap. By combining all the layers, it will be possible fx to compare areas of high restaurant density and high pedestrian density with predicted survival outcomes, helping to identify not only where restaurants are concentrated, but also where they are most likely to succeed over time.\n",
    "\n",
    "This first heatmap was obtained by combining the restaurants/bars dataset obtained in the first 2 steps and the Pedestrian Dataset downloaed from OpenData.dk\n",
    "\n",
    "Code key functionalities:\n",
    "- Restaurant Heatmap: Shows the density of restaurant locations.\n",
    "- Longevity Heatmap: Visualizes how long restaurants have stayed open, based on registration and closure dates.\n",
    "- Status Markers: Differentiates between currently active and closed restaurants with green and red markers.\n",
    "- Branchekode Filter: Allows filtering restaurants by industry classification code (branchekode).\n",
    "- Pedestrian Traffic Heatmap: Displays average daily foot traffic (7 AM–7 PM) from official measurements.\n",
    "- Peak Hour Traffic Circles: Highlights high-density areas during peak foot traffic (7 AM–7 PM) using proportional red circles.\n",
    "\n",
    "The result is an interactive map saved as an HTML file, enabling users to toggle layers, explore patterns, and identify high-potential zones for business development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7aa8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Load restaurant dataset with coordinates\n",
    "restaurants_df = pd.read_csv(\"scraped_companies_combined_clean_with_coords.csv\")\n",
    "restaurants_df = restaurants_df.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "# Load pedestrian traffic dataset with coordinates\n",
    "traffic_df = pd.read_csv(\"foot_trafic.csv\")\n",
    "traffic_df = traffic_df.dropna(subset=['lat', 'lon'])\n",
    "\n",
    "# Preprocessing longevity\n",
    "restaurants_df['startdate'] = pd.to_datetime(restaurants_df['startdate'], errors='coerce')\n",
    "restaurants_df['enddate'] = pd.to_datetime(restaurants_df['enddate'], errors='coerce')\n",
    "restaurants_df['enddate_filled'] = restaurants_df['enddate'].fillna(pd.Timestamp.today())\n",
    "restaurants_df['longevity_days'] = (restaurants_df['enddate_filled'] - restaurants_df['startdate']).dt.days\n",
    "\n",
    "# Initialize the map centered on Copenhagen\n",
    "map_ = folium.Map(location=[55.6761, 12.5683], zoom_start=13)\n",
    "\n",
    "# --- HEATMAP: Restaurants ---\n",
    "heat_points = restaurants_df[['latitude', 'longitude']].values.tolist()\n",
    "heatmap_layer = folium.FeatureGroup(name=\"Restaurants Heatmap\")\n",
    "HeatMap(heat_points, radius=10, blur=15).add_to(heatmap_layer)\n",
    "heatmap_layer.add_to(map_)\n",
    "\n",
    "# --- HEATMAP: Longevity ---\n",
    "longevity_points = restaurants_df[['latitude', 'longitude', 'longevity_days']].dropna().values.tolist()\n",
    "longevity_layer = folium.FeatureGroup(name=\"Restaurants Longevity Heatmap\", show=False)\n",
    "HeatMap(longevity_points, radius=15, blur=25, max_zoom=14).add_to(longevity_layer)\n",
    "longevity_layer.add_to(map_)\n",
    "\n",
    "# --- MARKERS: Active / Closed Restaurants ---\n",
    "active_layer = folium.FeatureGroup(name=\"Active Restaurants\", show=False)\n",
    "closed_layer = folium.FeatureGroup(name=\"Closed Restaurants\", show=False)\n",
    "\n",
    "for _, row in restaurants_df.iterrows():\n",
    "    popup = folium.Popup(\n",
    "        f\"<b>{row.get('name', 'N/A')}</b><br>\"\n",
    "        f\"Business Code: {row.get('branchekode', 'N/A')}<br>\"\n",
    "        f\"Status: {row.get('status', 'N/A')}<br>\"\n",
    "        f\"Opening Date: {row.get('startdate', 'N/A')}<br>\"\n",
    "        f\"Postal Code: {row.get('zip', 'N/A')}\",\n",
    "        max_width=300\n",
    "    )\n",
    "    marker = folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=popup,\n",
    "        icon=folium.Icon(color=\"green\" if row.get('active', False) else \"red\")\n",
    "    )\n",
    "    if row.get('active', False):\n",
    "        marker.add_to(active_layer)\n",
    "    else:\n",
    "        marker.add_to(closed_layer)\n",
    "\n",
    "active_layer.add_to(map_)\n",
    "closed_layer.add_to(map_)\n",
    "\n",
    "# --- FILTER: Branchekode ---\n",
    "branche_layer_dict = {}\n",
    "for branche in restaurants_df['branchekode'].dropna().unique():\n",
    "    layer = folium.FeatureGroup(name=f\"Branchekode: {branche}\", show=False)\n",
    "    for _, row in restaurants_df[restaurants_df['branchekode'] == branche].iterrows():\n",
    "        popup = folium.Popup(\n",
    "            f\"<b>{row.get('name', 'N/A')}</b><br>\"\n",
    "            f\"Branchekode: {row.get('branchekode', 'N/A')}<br>\"\n",
    "            f\"Status: {row.get('status', 'N/A')}<br>\"\n",
    "            f\"Startdate: {row.get('startdate', 'N/A')}<br>\"\n",
    "            f\"ZIP: {row.get('zip', 'N/A')}\",\n",
    "            max_width=300\n",
    "        )\n",
    "        folium.CircleMarker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            radius=4,\n",
    "            color=\"blue\",\n",
    "            fill=True,\n",
    "            fill_opacity=0.6,\n",
    "            popup=popup\n",
    "        ).add_to(layer)\n",
    "    layer.add_to(map_)\n",
    "\n",
    "# --- PEDESTRIAN TRAFFIC: HEATMAP aadt_fod_7_19 ---\n",
    "heat_traffic_points = traffic_df[['lat', 'lon', 'aadt_fod_7_19']].dropna().values.tolist()\n",
    "heatmap_ped_layer = folium.FeatureGroup(name=\"Pedestrian Heatmap (7-19)\")\n",
    "HeatMap(heat_traffic_points, radius=15, blur=25, max_zoom=14).add_to(heatmap_ped_layer)\n",
    "heatmap_ped_layer.add_to(map_)\n",
    "\n",
    "# --- PEDESTRIAN TRAFFIC: CIRCLE LAYER hvdt_fod_7_19 ---\n",
    "circle_layer = folium.FeatureGroup(name=\"Pedestrian Peak Hour 7-19\", show=False)\n",
    "for _, row in traffic_df.iterrows():\n",
    "    value = row.get('hvdt_fod_7_19')\n",
    "    if pd.notna(value):\n",
    "        radius = value / 500  # scaling factor\n",
    "        popup = folium.Popup(\n",
    "            f\"<b>{row.get('vejnavn', '')}</b><br>\"\n",
    "            f\"Peak Hour 7-19: {int(value)}<br>\"\n",
    "            f\"Description: {row.get('beskrivelse', '')}<br>\"\n",
    "            f\"Date: {row.get('taelle_dato', '')}\",\n",
    "            max_width=300\n",
    "        )\n",
    "        folium.CircleMarker(\n",
    "            location=[row['lat'], row['lon']],\n",
    "            radius=radius,\n",
    "            color=\"red\",\n",
    "            fill=True,\n",
    "            fill_opacity=0.5,\n",
    "            popup=popup\n",
    "        ).add_to(circle_layer)\n",
    "circle_layer.add_to(map_)\n",
    "\n",
    "# Add layer control to enable toggling layers\n",
    "folium.LayerControl(collapsed=False).add_to(map_)\n",
    "\n",
    "# Save the final map\n",
    "map_.save(\"interactive_map_with_filtered_traffic.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8b77d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stayingalive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
