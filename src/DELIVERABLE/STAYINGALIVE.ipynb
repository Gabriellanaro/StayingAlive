{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8681ab4e",
   "metadata": {},
   "source": [
    "# Stayin' Alive\n",
    "### *An AI-Powered Tool for Optimal Restaurant and Bar Location Selection and Business Longevity*\n",
    "42578 – Advanced Business Analytics, DTU, 2025 <br>\n",
    "Group 21 - Crocs Validation<br>\n",
    "Giulia Andreatta -sxxxx<br>\n",
    "Gabriel Lanaro - s233541<br>\n",
    "Alessia Saccardo - s212246<br>\n",
    "Gabriele Turetta - sxxxx<br>\n",
    "\n",
    "### Objective\n",
    "Opening a restaurant or a bar is a high-risk endeavor—many establishments close within their first few years. In Copenhagen, aspiring restaurateurs and investors often lack a data-driven approach when selecting a location. Moreover, understanding the reasons behind a restaurant’s success or failure remains a challenge.\n",
    "\n",
    "This project aims to:\n",
    "\n",
    "- Recommend optimal locations for new restaurants or bars using Survival Analysis.\n",
    "- Visualize location suitability through an interactive heatmap enriched with predictive longevity scores, pedestrian peak hours, density of restaurants, pins of active and closed activities.\n",
    "\n",
    "### Datasets\n",
    "\n",
    "- **Company data scraped from the official CVR registry via [virk.dk](https://datacvr.virk.dk/soegeresultater?fritekst=d&sideIndex=0&size=10)**<br>\n",
    "Includes business registration details, location, restaurant closures, branchekode.\n",
    "\n",
    "- **Google Maps Scraped Data**<br>\n",
    "Includes business location, rating, number of reviews, price range, tags.\n",
    "\n",
    "- **Pedestrian Dataset from [OpenData.dk](https://www.opendata.dk/city-of-copenhagen/taelling_fodg#:~:text=Number%20of%20pedestrians%20counted%20on,19%20in%20both%20directions)**<br>\n",
    "Provides foot traffic counts recorded at specific times and locations in Copenhagen.\n",
    "\n",
    "- **People per Postal Code and Neighbordhood Dataset [Statbank.dk](https://www.statbank.dk/20021)**<br>\n",
    "Provides the Population at the 1st of January 2025 by municipality, postal code, sex and age.\n",
    "\n",
    "- **Area per postal code In Copenhagen [Postnumre-da.cybo](https://postnumre-da.cybo.com/danmark/k%C3%B8benhavn/)<br>**\n",
    "Contains a lot of information about the postal codes but we took only the area of each one.\n",
    "\n",
    "### ABA Topics Covered\n",
    "- **Web Data Mining**\n",
    "Scraping large-scale data from Google Maps and government databases to construct the datasets.\n",
    "\n",
    "- **Survival Analysis**\n",
    "Predicting restaurant longevity using Kaplan-Meier and Cox Proportional Hazards models.\n",
    "\n",
    "- **Recommender Systems**\n",
    "Suggesting location options for new restaurants and bars based on market gaps and existing competition.\n",
    "\n",
    "- **AI in the Real World**\n",
    "Delivering real value to stakeholders by supporting data-driven restaurant planning and resilience strategies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bab528",
   "metadata": {},
   "source": [
    "## 1st Step - Data Scraping from the official CVR registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41cae2e",
   "metadata": {},
   "source": [
    "This script performs web scraping on the Danish company registry website (https://datacvr.virk.dk)\n",
    "to extract company details for active business units in specified industry sectors (branchekoder).\n",
    "It uses Selenium to navigate the search results, extract key information for each business unit,\n",
    "and follow links to detailed company pages to obtain start and end dates.\n",
    "\n",
    "The results are saved in a CSV file, and duplicate entries (based on P-number) are avoided by \n",
    "keeping track of already seen values. The script is designed to be resumed without duplicating \n",
    "previous entries.\n",
    "\n",
    "Required: chromedriver installed and path correctly set.\n",
    "\n",
    "Note: The output of the 1st and 2nd cells are already present in the data folder. If you want to run the scraping cells, take in mind that the run takes a very long time!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "driver_path = r\"chromedriver/chromedriver.exe\"\n",
    "options = Options()\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(driver_path), options=options)\n",
    "\n",
    "\n",
    "# the loop can be run for each branchekode separately, or all at once by uncommenting the lines below.\n",
    "# In our project, the branchekodes were manually uncommented, to create a single csv file for each branchekode.\n",
    "# This was done to avoid the script from running for too long, risking on IP bans or website crashes.\n",
    "# The single csv files were then merged into a single csv file.\n",
    "branchekodes = [\n",
    "    # 561110,   # serving food in restaurants and cafes\n",
    "    # 561190,   # includes the operation of restaurants, where the main emphasis is on takeaway with very limited table service.\n",
    "    # 563010,   # includes serving beverages, possibly with some edibles, but where the main emphasis is on serving non-alcoholic beverages for immediate consumption on site.\n",
    "    563020,  #  includes serving beverages, possibly with some edibles, but where the main emphasis is on serving alcoholic beverages for immediate consumption on the premises.\n",
    "]\n",
    "\n",
    "# Scraping Structure overview:\n",
    "# - Main search results are loaded via URL with parameters: sideIndex (pagination), branchekode (industry), etc.\n",
    "# - Each company entry is a 'div.row' within a 'div[data-cy=\"soegeresultater-tabel\"]'.\n",
    "# - Basic info (name, address, P-nummer, status, company type) is extracted directly from the search results.\n",
    "# - For each company, the script follows the \"Show More\" link in a new browser tab to extract Start date (Startdato)\n",
    "#   and End date (Ophørsdato), which appear in divs following label tags (either <strong> or <span>).\n",
    "for branchekode in branchekodes:\n",
    "    page = 0\n",
    "    csv_file_path = f\"data/scraped_companies_{branchekode}.csv\"\n",
    "    header = [\n",
    "        \"Name\",\n",
    "        \"Address\",\n",
    "        \"P-nummer\",\n",
    "        \"Status\",\n",
    "        \"Company Type\",\n",
    "        \"Startdate\",\n",
    "        \"Enddate\",\n",
    "    ]\n",
    "    pnummer_seen = set()\n",
    "\n",
    "    # Load existing P-numbers if the file already exists to avoid duplicates\n",
    "    file_exists = os.path.exists(csv_file_path)\n",
    "    if file_exists:\n",
    "        with open(csv_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                pnummer_seen.add(row[\"P-nummer\"])\n",
    "    else:\n",
    "        # Create file and write header\n",
    "        with open(csv_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=header)\n",
    "            writer.writeheader()\n",
    "\n",
    "    while True:\n",
    "        url = f\"https://datacvr.virk.dk/soegeresultater?sideIndex={page}&enhedstype=produktionsenhed&region=29190623&branchekode={branchekode}\"\n",
    "        print(f\"Scraping page {page}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Get all company rows from the result table\n",
    "        rows = driver.find_elements(\n",
    "            By.CSS_SELECTOR, 'div[data-cy=\"soegeresultater-tabel\"] > div.row'\n",
    "        )\n",
    "\n",
    "        if not rows:\n",
    "            print(\"No data found. Stopping.\")\n",
    "            break\n",
    "\n",
    "        # Process each company in the current page\n",
    "        for row in rows:\n",
    "            try:\n",
    "                name = row.find_element(By.CSS_SELECTOR, \"span.bold.value\").text.strip()\n",
    "\n",
    "                address_block = row.find_element(By.CSS_SELECTOR, \"div.col-12.col-lg-4\")\n",
    "                address_lines = address_block.text.strip().split(\"\\n\")[-2:]\n",
    "                address = \", \".join(address_lines)\n",
    "\n",
    "                pnummer = row.find_element(\n",
    "                    By.XPATH, './/div[div[text()=\"P-nummer:\"]]/div[2]'\n",
    "                ).text.strip()\n",
    "\n",
    "                # Skip if already saved\n",
    "                if pnummer in pnummer_seen:\n",
    "                    continue\n",
    "                pnummer_seen.add(pnummer)\n",
    "\n",
    "                status = row.find_element(\n",
    "                    By.XPATH, './/div[div[text()=\"Status:\"]]/div[2]'\n",
    "                ).text.strip()\n",
    "                form = row.find_element(\n",
    "                    By.XPATH, './/div[div[text()=\"Virksomhedsform:\"]]/div[2]'\n",
    "                ).text.strip()\n",
    "\n",
    "                link_elem = row.find_element(\n",
    "                    By.CSS_SELECTOR, 'div[data-cy=\"vis-mere\"] a'\n",
    "                )\n",
    "                link = link_elem.get_attribute(\"href\")\n",
    "\n",
    "                # Open detail page in new tab\n",
    "                driver.execute_script(\"window.open('');\")\n",
    "                driver.switch_to.window(driver.window_handles[1])\n",
    "                driver.get(link)\n",
    "                time.sleep(3)\n",
    "\n",
    "                # Extract dates\n",
    "                startdato = \"\"\n",
    "                ophoersdato = \"\"\n",
    "\n",
    "                # Extract start and end dates from the detail page\n",
    "                try:\n",
    "                    startdato_element = driver.find_element(\n",
    "                        By.XPATH,\n",
    "                        '//div[(strong[text()=\"Startdato\"] or span[text()=\"Startdato\"])]/following-sibling::div',\n",
    "                    )\n",
    "                    startdato = startdato_element.text.strip()\n",
    "                except:\n",
    "                    startdato = \"\"\n",
    "\n",
    "                try:\n",
    "                    ophoersdato_element = driver.find_element(\n",
    "                        By.XPATH,\n",
    "                        '//div[(strong[text()=\"Ophørsdato\"] or span[text()=\"Ophørsdato\"])]/following-sibling::div',\n",
    "                    )\n",
    "                    ophoersdato = ophoersdato_element.text.strip()\n",
    "                except:\n",
    "                    ophoersdato = \"\"\n",
    "\n",
    "                # Close the detail tab and return to the main results tab\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "                # Write to CSV\n",
    "                with open(csv_file_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                    writer = csv.DictWriter(f, fieldnames=header)\n",
    "                    writer.writerow(\n",
    "                        {\n",
    "                            \"Name\": name,\n",
    "                            \"Address\": address,\n",
    "                            \"P-nummer\": pnummer,\n",
    "                            \"Status\": status,\n",
    "                            \"Company Type\": form,\n",
    "                            \"Startdate\": startdato,\n",
    "                            \"Enddate\": ophoersdato,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                print(f\"{name} | {startdato} → {ophoersdato}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Error during parsing:\", e)\n",
    "                continue\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "driver.quit()\n",
    "print(\"Scraping finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81ae89",
   "metadata": {},
   "source": [
    "## 2nd Step - Geocoding Restaurant Addresses Using OpenStreetMap API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cae29c",
   "metadata": {},
   "source": [
    "This script performs address geocoding using the OpenStreetMap (OSM) API via the geopy library. It takes as input a CSV file containing restaurant records with address fields but missing geographic coordinates (data/scraped_companies_combined_clean.csv). For each address, it attempts to retrieve the corresponding latitude and longitude, which are then saved in a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ec83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "input_file = \"data/scraped_companies_combined_clean.csv\"\n",
    "output_file = \"data/scraped_companies_combined_clean_with_coords.csv\"\n",
    "\n",
    "# === LOAD ORIGINAL DATA ===\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "\n",
    "# Function to simplify the address before geocoding\n",
    "def simplify_address(row):\n",
    "    addr = str(row[\"Address\"])\n",
    "    addr = addr.split(\",\")[0].strip()  # only keep the part before the first comma\n",
    "    return f\"{addr}, Denmark\"\n",
    "\n",
    "\n",
    "# Add coordinate columns if they don't exist\n",
    "if \"latitude\" not in df.columns:\n",
    "    df[\"latitude\"] = None\n",
    "if \"longitude\" not in df.columns:\n",
    "    df[\"longitude\"] = None\n",
    "\n",
    "# Load already geocoded addresses to avoid duplicates\n",
    "already_done = set()\n",
    "if os.path.exists(output_file):\n",
    "    df_existing = pd.read_csv(output_file)\n",
    "    already_done = set(df_existing[\"Address\"].dropna().unique())\n",
    "    print(f\"Resuming from {len(already_done)} already completed addresses.\")\n",
    "\n",
    "# Filter rows that still need geocoding\n",
    "df_to_process = df[~df[\"Address\"].isin(already_done)].copy()\n",
    "print(f\"Addresses to geocode: {len(df_to_process)}\")\n",
    "\n",
    "# Initialize OpenStreetMap geocoder with delay to respect rate limits\n",
    "geolocator = Nominatim(user_agent=\"stayin_alive_simple_geocoder\")\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1.5)\n",
    "\n",
    "# Progressive saving to CSV (append mode!!)\n",
    "with open(output_file, \"a\", encoding=\"utf-8\", newline=\"\") as f_out:\n",
    "    header_written = os.stat(output_file).st_size == 0\n",
    "    for i, row in df_to_process.iterrows():\n",
    "        full_address = simplify_address(row)\n",
    "        try:\n",
    "            location = geocode(full_address)\n",
    "            if location:\n",
    "                row[\"latitude\"] = location.latitude\n",
    "                row[\"longitude\"] = location.longitude\n",
    "                print(f\"{full_address} -> ({location.latitude}, {location.longitude})\")\n",
    "            else:\n",
    "                print(f\"{full_address} -> not found\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error on {full_address}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Append row to output CSV\n",
    "        pd.DataFrame([row]).to_csv(f_out, index=False, header=header_written)\n",
    "        header_written = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d13553",
   "metadata": {},
   "source": [
    "## 3rd Step - Scraping Restaurant Metadata from Google Maps with Selenium\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad7e0b2",
   "metadata": {},
   "source": [
    "This script performs web scraping from Google Maps using Selenium to enrich the dataset of restaurants obtained from steps 1-2 (data/scraped_companies_combined_clean_with_coords.csv). For each restaurant entry (name and address), the script:\n",
    "\n",
    "1. Opens a Google Maps search page\n",
    "\n",
    "2. Extracts the official listing title, star rating, number of reviews, price level, and associated category tags if present\n",
    "\n",
    "3. Saves the collected data into a CSV file\n",
    "\n",
    "4. The script supports resuming interrupted sessions by skipping entries that have already been saved to the output CSV file.\n",
    "\n",
    "Required: chromedriver installed and path correctly set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "driver_path = r\"chromedriver/chromedriver.exe\"\n",
    "options = Options()\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "# options.add_argument(\"--headless\")  # Uncomment to run without opening browser window\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(driver_path), options=options)\n",
    "\n",
    "# === INPUT & OUTPUT PATHS ===\n",
    "csv_input_path = r\"data/scraped_companies_combined_clean_with_coords.csv\"\n",
    "csv_output_path = r\"data/maps_data_scraped.csv\"\n",
    "\n",
    "# Load input data\n",
    "df_input = pd.read_csv(csv_input_path)\n",
    "restaurant_data = df_input.to_dict(orient=\"records\")\n",
    "\n",
    "# Load already saved entries (if output file exists)\n",
    "saved_entries = set()\n",
    "header = [\n",
    "    \"Input Name\",\n",
    "    \"Input Address\",\n",
    "    \"Title\",\n",
    "    \"Rating\",\n",
    "    \"Reviews\",\n",
    "    \"Price Level\",\n",
    "    \"Tags\",\n",
    "]\n",
    "\n",
    "if os.path.exists(csv_output_path):\n",
    "    with open(csv_output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            key = (row[\"Input Name\"], row[\"Input Address\"])\n",
    "            saved_entries.add(key)\n",
    "else:\n",
    "    with open(csv_output_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "\n",
    "# === MAIN LOOP OVER RESTAURANTS ===\n",
    "for entry in restaurant_data:\n",
    "    name = entry[\"Name\"]\n",
    "    address = entry[\"Address\"]\n",
    "    key = (name, address)\n",
    "\n",
    "    if key in saved_entries:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"Searching: {name} @ {address}\")\n",
    "        query = f\"{name} {address}\".replace(\" \", \"+\")\n",
    "        linkmaps = f\"https://www.google.com/maps/search/{query}\"\n",
    "        print(f\"URL: {linkmaps}\")\n",
    "        driver.get(linkmaps)\n",
    "        time.sleep(2)\n",
    "\n",
    "        try:\n",
    "            title = driver.find_element(By.CSS_SELECTOR, \"h1.DUwDvf\").text\n",
    "        except:\n",
    "            title = \"\"\n",
    "\n",
    "        try:\n",
    "            rating = driver.find_element(\n",
    "                By.CSS_SELECTOR, 'div.F7nice > span span[aria-hidden=\"true\"]'\n",
    "            ).text\n",
    "        except:\n",
    "            rating = \"\"\n",
    "\n",
    "        try:\n",
    "            reviews_elem = driver.find_element(\n",
    "                By.CSS_SELECTOR, 'div.F7nice > span span[aria-label$=\"reviews\"]'\n",
    "            ).text\n",
    "            reviews = reviews_elem.strip(\"()\")\n",
    "        except:\n",
    "            reviews = \"\"\n",
    "\n",
    "        try:\n",
    "            price_level = driver.find_element(\n",
    "                By.CSS_SELECTOR, \"div.DfOCNb.fontBodyMedium > div\"\n",
    "            ).text.split(\"\\n\")[0]\n",
    "        except:\n",
    "            price_level = \"\"\n",
    "\n",
    "        try:\n",
    "            outer_divs = driver.find_elements(By.CSS_SELECTOR, \"div.KNfEk.aUjao\")\n",
    "            tags = []\n",
    "            for div in outer_divs:\n",
    "                try:\n",
    "                    tag = div.find_element(\n",
    "                        By.CSS_SELECTOR, \"div.tXNTee span.uEubGf.fontBodyMedium\"\n",
    "                    ).text\n",
    "                    tags.append(tag)\n",
    "                except:\n",
    "                    continue\n",
    "            tags = \", \".join(tags)\n",
    "        except:\n",
    "            tags = \"\"\n",
    "\n",
    "        # Append to CSV immediately\n",
    "        with open(csv_output_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=header)\n",
    "            writer.writerow(\n",
    "                {\n",
    "                    \"Input Name\": name,\n",
    "                    \"Input Address\": address,\n",
    "                    \"Title\": title,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Reviews\": reviews,\n",
    "                    \"Price Level\": price_level,\n",
    "                    \"Tags\": tags,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        print(f\"Saved: {title}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during scraping:\", e)\n",
    "        continue\n",
    "\n",
    "driver.quit()\n",
    "print(\"Scraping finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f50eb",
   "metadata": {},
   "source": [
    "## 4th Step – Cleaning and Preparing the Population by Postal Code Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9865618",
   "metadata": {},
   "source": [
    "The original dataset included the following fields: **ID**, **Neighborhood**, **Total People**, **Total Men**, and **Total Women**. Preliminary data cleaning was performed primarily in Excel for convenience. The key steps were:\n",
    "\n",
    "1. The **Neighborhood** column was split into two separate fields: **Postal Code** and **Neighborhood Name**, resulting in the file `cph_population_clean.csv`.\n",
    "2. This cleaned dataset was merged with a postal code area reference table to enable the calculation of population density per postal code.\n",
    "3. Population and restaurant densities were calculated, producing the final dataset `population_df.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "549d2e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge completed. File saved as merged_companies.csv\n",
      "   neighborhood_code neighborhood_name postal_code  postal_area  \\\n",
      "0                101         København        1050  København K   \n",
      "1                101         København        1051  København K   \n",
      "2                101         København        1052  København K   \n",
      "3                101         København        1053  København K   \n",
      "4                101         København        1054  København K   \n",
      "\n",
      "   postal_code_m2  Total  Men  Women  population_density_km2  restaurant_count  \n",
      "0           39053     20   10     10              512.124549                16  \n",
      "1           55643    399  191    208             7170.713297                28  \n",
      "2            6563    421  204    217            64147.493524                 2  \n",
      "3            2434    404  181    223           165981.922761                 2  \n",
      "4            4953    392  201    191            79143.953160                 1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Step 1: Load cleaned population dataset\n",
    "population_df = pd.read_csv(\n",
    "    \"data/cph_population_clean.csv\",\n",
    "    encoding=\"ISO-8859-1\",\n",
    ")\n",
    "\n",
    "# Step 2: Collect all company data CSVs from the specified folder\n",
    "# These files follow a naming convention like scraped_companies_<code>_active.csv\n",
    "file_paths = glob.glob(\"data/Temp/scraped_companies_*.csv\")\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "# Step 3: Loop through each company CSV file\n",
    "for path in file_paths:\n",
    "    filename = os.path.basename(path)\n",
    "\n",
    "    # Extract the company identifier code from the filename\n",
    "    try:\n",
    "        code = filename.split(\"_\")[2]\n",
    "    except IndexError:\n",
    "        code = \"000000\"  # Use fallback code if filename is not in expected format\n",
    "\n",
    "    # Determine if the file refers to active companies\n",
    "    is_active = filename.lower().endswith(\"active.csv\")\n",
    "\n",
    "    # Load the current CSV file\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Add identifier and status columns to the DataFrame\n",
    "    df[\"code\"] = code\n",
    "    df[\"active\"] = is_active\n",
    "\n",
    "    # Store in the list for later concatenation\n",
    "    all_dfs.append(df)\n",
    "\n",
    "# Step 4: Combine all company DataFrames into one\n",
    "merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Optional: Save merged file (currently commented out)\n",
    "# merged_df.to_csv(\"merged_companies.csv\", index=False)\n",
    "\n",
    "print(\"Merge completed. File saved as merged_companies.csv\")\n",
    "\n",
    "\n",
    "# Step 5: Extract the postal code from the Address column using regex\n",
    "def extract_postal_code(address):\n",
    "    try:\n",
    "        # Look for a pattern like \", 2100\" where 2100 is the postal code\n",
    "        match = re.search(r\",\\s*(\\d{4})\", address)\n",
    "        return match.group(1) if match else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Apply the postal code extraction function\n",
    "merged_df[\"postal_code\"] = merged_df[\"Address\"].apply(extract_postal_code)\n",
    "\n",
    "# Drop columns that are not needed for the analysis\n",
    "merged_df = merged_df.drop(columns=[\"Status\"])\n",
    "merged_df = merged_df.drop(columns=[\"Company Type\"])\n",
    "\n",
    "# Standardize column names and formats for merging\n",
    "population_df = population_df.rename(columns={\"neighborhood_id\": \"neighborhood_code\"})\n",
    "population_df = population_df.replace(\"-\", 0)  # Replace dashes with 0 (if any)\n",
    "\n",
    "# Ensure postal codes are strings to prevent merge issues\n",
    "population_df[\"postal_code\"] = population_df[\"postal_code\"].astype(str)\n",
    "population_df[\"postal_code_m2\"] = population_df[\"postal_code_m2\"].astype(int)\n",
    "merged_df[\"postal_code\"] = merged_df[\"postal_code\"].astype(str)\n",
    "\n",
    "# Step 6: Compute population density (people per km²)\n",
    "population_df[\"population_density_km2\"] = population_df[\"Total\"] / (\n",
    "    population_df[\"postal_code_m2\"] / 1_000_000\n",
    ")\n",
    "\n",
    "# Step 7: Count how many restaurants (companies) exist per postal code\n",
    "restaurant_counts = (\n",
    "    merged_df.groupby(\"postal_code\").size().reset_index(name=\"restaurant_count\")\n",
    ")\n",
    "\n",
    "# Step 8: Merge restaurant counts into the population DataFrame\n",
    "population_df = population_df.merge(restaurant_counts, on=\"postal_code\", how=\"left\")\n",
    "\n",
    "# Fill any missing values (postal codes with 0 restaurants) with 0\n",
    "population_df[\"restaurant_count\"] = (\n",
    "    population_df[\"restaurant_count\"].fillna(0).astype(int)\n",
    ")\n",
    "\n",
    "# Step 9: Save final dataset with population and restaurant density info\n",
    "population_df.to_csv(\"data/population_df.csv\", index=False)\n",
    "\n",
    "# Print a sample of the final DataFrame\n",
    "print(population_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741168c",
   "metadata": {},
   "source": [
    "## 5th Step - 1st Heatmap, an interactive Spatial Visualization of Restaurants/Bars and Pedestrian Traffic in Copenhagen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5985d62b",
   "metadata": {},
   "source": [
    "This script generates an interactive Folium heatmap that visualizes restaurant/bar locations and pedestrian traffic in Copenhagen. It combines multiple layers of spatial data to support exploratory analysis for business location decisions. \n",
    "This initial heatmap with multiple layers serves as a visual foundation for later overlaying the survival analysis scores as a new layer in the heatmap. By combining all the layers, it will be possible fx to compare areas of high restaurant density and high pedestrian density with predicted survival outcomes, helping to identify not only where restaurants are concentrated, but also where they are most likely to succeed over time.\n",
    "\n",
    "This first heatmap was obtained by combining the restaurants/bars dataset obtained in the first 2 steps (data/scraped_companies_combined_clean_with_coords.csv) and the Pedestrian Dataset downloaed from OpenData.dk (data/foot_traffic.csv)\n",
    "\n",
    "Code key functionalities:\n",
    "- Restaurant Heatmap: Shows the density of restaurant locations.\n",
    "- Longevity Heatmap: Visualizes how long restaurants have stayed open, based on registration and closure dates.\n",
    "- Status Markers: Differentiates between currently active and closed restaurants with green and red markers.\n",
    "- Branchekode Filter: Allows filtering restaurants by industry classification code (branchekode).\n",
    "- Pedestrian Traffic Heatmap: Displays average daily foot traffic (7 AM–7 PM) from official measurements.\n",
    "- Peak Hour Traffic Circles: Highlights high-density areas during peak foot traffic (7 AM–7 PM) using proportional red circles.\n",
    "\n",
    "The result is an interactive map saved as an HTML file, enabling users to toggle layers, explore patterns, and identify high-potential zones for business development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7aa8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Load restaurant dataset with coordinates\n",
    "restaurants_df = pd.read_csv(\"data/scraped_companies_combined_clean_with_coords.csv\")\n",
    "restaurants_df = restaurants_df.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "\n",
    "# Load pedestrian traffic dataset with coordinates\n",
    "traffic_df = pd.read_csv(\"data/foot_trafic.csv\")\n",
    "traffic_df = traffic_df.dropna(subset=[\"lat\", \"lon\"])\n",
    "\n",
    "# Preprocessing longevity\n",
    "restaurants_df[\"startdate\"] = pd.to_datetime(\n",
    "    restaurants_df[\"startdate\"], errors=\"coerce\"\n",
    ")\n",
    "restaurants_df[\"enddate\"] = pd.to_datetime(restaurants_df[\"enddate\"], errors=\"coerce\")\n",
    "restaurants_df[\"enddate_filled\"] = restaurants_df[\"enddate\"].fillna(\n",
    "    pd.Timestamp.today()\n",
    ")\n",
    "restaurants_df[\"longevity_days\"] = (\n",
    "    restaurants_df[\"enddate_filled\"] - restaurants_df[\"startdate\"]\n",
    ").dt.days\n",
    "\n",
    "# Initialize the map centered on Copenhagen\n",
    "map_ = folium.Map(location=[55.6761, 12.5683], zoom_start=13)\n",
    "\n",
    "# --- HEATMAP: Restaurants ---\n",
    "heat_points = restaurants_df[[\"latitude\", \"longitude\"]].values.tolist()\n",
    "heatmap_layer = folium.FeatureGroup(name=\"Restaurants Heatmap\")\n",
    "HeatMap(heat_points, radius=10, blur=15).add_to(heatmap_layer)\n",
    "heatmap_layer.add_to(map_)\n",
    "\n",
    "# --- HEATMAP: Longevity ---\n",
    "longevity_points = (\n",
    "    restaurants_df[[\"latitude\", \"longitude\", \"longevity_days\"]].dropna().values.tolist()\n",
    ")\n",
    "longevity_layer = folium.FeatureGroup(name=\"Restaurants Longevity Heatmap\", show=False)\n",
    "HeatMap(longevity_points, radius=15, blur=25, max_zoom=14).add_to(longevity_layer)\n",
    "longevity_layer.add_to(map_)\n",
    "\n",
    "# --- MARKERS: Active / Closed Restaurants ---\n",
    "active_layer = folium.FeatureGroup(name=\"Active Restaurants\", show=False)\n",
    "closed_layer = folium.FeatureGroup(name=\"Closed Restaurants\", show=False)\n",
    "\n",
    "for _, row in restaurants_df.iterrows():\n",
    "    popup = folium.Popup(\n",
    "        f\"<b>{row.get('name', 'N/A')}</b><br>\"\n",
    "        f\"Business Code: {row.get('branchekode', 'N/A')}<br>\"\n",
    "        f\"Status: {row.get('status', 'N/A')}<br>\"\n",
    "        f\"Opening Date: {row.get('startdate', 'N/A')}<br>\"\n",
    "        f\"Postal Code: {row.get('zip', 'N/A')}\",\n",
    "        max_width=300,\n",
    "    )\n",
    "    marker = folium.Marker(\n",
    "        location=[row[\"latitude\"], row[\"longitude\"]],\n",
    "        popup=popup,\n",
    "        icon=folium.Icon(color=\"green\" if row.get(\"active\", False) else \"red\"),\n",
    "    )\n",
    "    if row.get(\"active\", False):\n",
    "        marker.add_to(active_layer)\n",
    "    else:\n",
    "        marker.add_to(closed_layer)\n",
    "\n",
    "active_layer.add_to(map_)\n",
    "closed_layer.add_to(map_)\n",
    "\n",
    "# --- FILTER: Branchekode ---\n",
    "branche_layer_dict = {}\n",
    "for branche in restaurants_df[\"branchekode\"].dropna().unique():\n",
    "    layer = folium.FeatureGroup(name=f\"Branchekode: {branche}\", show=False)\n",
    "    for _, row in restaurants_df[restaurants_df[\"branchekode\"] == branche].iterrows():\n",
    "        popup = folium.Popup(\n",
    "            f\"<b>{row.get('name', 'N/A')}</b><br>\"\n",
    "            f\"Branchekode: {row.get('branchekode', 'N/A')}<br>\"\n",
    "            f\"Status: {row.get('status', 'N/A')}<br>\"\n",
    "            f\"Startdate: {row.get('startdate', 'N/A')}<br>\"\n",
    "            f\"ZIP: {row.get('zip', 'N/A')}\",\n",
    "            max_width=300,\n",
    "        )\n",
    "        folium.CircleMarker(\n",
    "            location=[row[\"latitude\"], row[\"longitude\"]],\n",
    "            radius=4,\n",
    "            color=\"blue\",\n",
    "            fill=True,\n",
    "            fill_opacity=0.6,\n",
    "            popup=popup,\n",
    "        ).add_to(layer)\n",
    "    layer.add_to(map_)\n",
    "\n",
    "# --- PEDESTRIAN TRAFFIC: HEATMAP aadt_fod_7_19 ---\n",
    "heat_traffic_points = (\n",
    "    traffic_df[[\"lat\", \"lon\", \"aadt_fod_7_19\"]].dropna().values.tolist()\n",
    ")\n",
    "heatmap_ped_layer = folium.FeatureGroup(name=\"Pedestrian Heatmap (7-19)\")\n",
    "HeatMap(heat_traffic_points, radius=15, blur=25, max_zoom=14).add_to(heatmap_ped_layer)\n",
    "heatmap_ped_layer.add_to(map_)\n",
    "\n",
    "# --- PEDESTRIAN TRAFFIC: CIRCLE LAYER hvdt_fod_7_19 ---\n",
    "circle_layer = folium.FeatureGroup(name=\"Pedestrian Peak Hour 7-19\", show=False)\n",
    "for _, row in traffic_df.iterrows():\n",
    "    value = row.get(\"hvdt_fod_7_19\")\n",
    "    if pd.notna(value):\n",
    "        radius = value / 500  # scaling factor\n",
    "        popup = folium.Popup(\n",
    "            f\"<b>{row.get('vejnavn', '')}</b><br>\"\n",
    "            f\"Peak Hour 7-19: {int(value)}<br>\"\n",
    "            f\"Description: {row.get('beskrivelse', '')}<br>\"\n",
    "            f\"Date: {row.get('taelle_dato', '')}\",\n",
    "            max_width=300,\n",
    "        )\n",
    "        folium.CircleMarker(\n",
    "            location=[row[\"lat\"], row[\"lon\"]],\n",
    "            radius=radius,\n",
    "            color=\"red\",\n",
    "            fill=True,\n",
    "            fill_opacity=0.5,\n",
    "            popup=popup,\n",
    "        ).add_to(circle_layer)\n",
    "circle_layer.add_to(map_)\n",
    "\n",
    "# Add layer control to enable toggling layers\n",
    "folium.LayerControl(collapsed=False).add_to(map_)\n",
    "\n",
    "# Save the final map\n",
    "map_.save(\"output/interactive_map_with_filtered_traffic.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6520749c",
   "metadata": {},
   "source": [
    "The final aim of this project is to add a new layer to the existing heatmap that visualizes the results of the survival analysis. To perform the analysis and produce an output that could be displayed as a heatmap, it has been created a dataset in which each row corresponds to a unique geographic coordinate. The following sections describe the construction of the dataset used for the survival analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8b77d",
   "metadata": {},
   "source": [
    "## 6th Step - Expanding the Pedestrian Traffic Dataset to the entire Copenhagen municipality "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efbf48",
   "metadata": {},
   "source": [
    "This script expands the pedestrian traffic dataset to cover the entire Copenhagen municipality. The original dataset (data/foot_trafic.csv) contains point-based pedestrian counts collected at specific locations, including fields such as `aadt_fod_7_19` (average daily traffic between 07:00–19:00) and `hvdt_fod_7_19` (peak traffic during the same period). Only these two fields were used in this step. \n",
    "\n",
    "To generalize the data spatially, the script first generates a regular 200-meter grid across the municipality and then clips it to the official boundary using Copenhagen's GeoJSON shapefile. Each grid point is then assigned average daily and peak pedestrian counts using inverse-distance weighting (IDW), a method that computes weighted averages of nearby observations with weights proportional to \\(1/d^3\\). \n",
    "\n",
    "The result is a new dataset (data/expanded_foot_trafic.csv) with over 7 000 grid points, each spaced 200 meters apart, containing estimated values of average and peak foot traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e88a9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original pedestrian points: 403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8026/1067146639.py:33: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  cph_union = munis[munis[\"label_dk\"] == \"København\"].unary_union\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid points after clipping: 7062\n",
      "Saved interpolated grid.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "# Load and clean pedestrian counts\n",
    "ped = pd.read_csv(\"data/foot_trafic.csv\", parse_dates=[\"taelle_dato\"])\n",
    "ped[\"longitude\"] = ped[\"wkb_geometry\"].str.extract(r\"POINT \\(([^ ]+)\")[0].astype(float)\n",
    "ped[\"latitude\"] = (\n",
    "    ped[\"wkb_geometry\"].str.extract(r\"POINT \\([^ ]+ ([^ ]+)\\)\")[0].astype(float)\n",
    ")\n",
    "ped = ped.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "ped[\"aadt\"] = pd.to_numeric(ped[\"aadt_fod_7_19\"], errors=\"coerce\")\n",
    "ped[\"hvdt\"] = pd.to_numeric(ped[\"hvdt_fod_7_19\"], errors=\"coerce\")\n",
    "ped = ped.dropna(subset=[\"aadt\", \"hvdt\"])\n",
    "\n",
    "# Project to metric CRS for distance calculations\n",
    "ped_gdf = gpd.GeoDataFrame(\n",
    "    ped[[\"aadt\", \"hvdt\", \"longitude\", \"latitude\"]],\n",
    "    geometry=gpd.points_from_xy(ped.longitude, ped.latitude),\n",
    "    crs=\"EPSG:4326\",\n",
    ").to_crs(epsg=3857)\n",
    "\n",
    "print(f\"Original pedestrian points: {len(ped_gdf)}\")\n",
    "\n",
    "# Build 200 m grid over Copenhagen\n",
    "munis = gpd.read_file(\n",
    "    \"https://raw.githubusercontent.com/\"\n",
    "    \"magnuslarsen/geoJSON-Danish-municipalities/\"\n",
    "    \"master/municipalities/municipalities.geojson\"\n",
    ").to_crs(epsg=3857)\n",
    "cph_union = munis[munis[\"label_dk\"] == \"København\"].unary_union\n",
    "spacing = 200\n",
    "minx, miny, maxx, maxy = cph_union.bounds\n",
    "xs = np.arange(minx, maxx + spacing, spacing)\n",
    "ys = np.arange(miny, maxy + spacing, spacing)\n",
    "\n",
    "grid = (\n",
    "    gpd.GeoDataFrame(geometry=[Point(x, y) for x in xs for y in ys], crs=\"EPSG:3857\")\n",
    "    .loc[lambda df: df.within(cph_union)]\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Grid points after clipping: {len(grid)}\")\n",
    "\n",
    "\n",
    "# IDW interpolation function\n",
    "def idw(point, xs, ys, values, power=3):\n",
    "    d = np.hypot(xs - point.x, ys - point.y)\n",
    "    w = 1.0 / (d**power + 1e-6)\n",
    "    return (w * values).sum() / w.sum()\n",
    "\n",
    "\n",
    "# Prepare arrays for interpolation\n",
    "xs_pts = ped_gdf.geometry.x.values\n",
    "ys_pts = ped_gdf.geometry.y.values\n",
    "aadt_vals = ped_gdf[\"aadt\"].values\n",
    "hvdt_vals = ped_gdf[\"hvdt\"].values\n",
    "\n",
    "# Compute interpolated traffic counts\n",
    "grid[\"aadt_fod_7_19\"] = grid.geometry.apply(lambda p: idw(p, xs_pts, ys_pts, aadt_vals))\n",
    "grid[\"hvdt_fod_7_19\"] = grid.geometry.apply(lambda p: idw(p, xs_pts, ys_pts, hvdt_vals))\n",
    "\n",
    "# Reproject to lat/lon and save\n",
    "out = grid.to_crs(epsg=4326)\n",
    "out[\"latitude\"] = out.geometry.y\n",
    "out[\"longitude\"] = out.geometry.x\n",
    "out[[\"latitude\", \"longitude\", \"aadt_fod_7_19\", \"hvdt_fod_7_19\"]].to_csv(\n",
    "    \"data/expanded_foot_trafic.csv\", index=False\n",
    ")\n",
    "\n",
    "print(\"Saved interpolated grid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae2afbf",
   "metadata": {},
   "source": [
    "## 7th Step - Merge Pedestrian Traffic with Postal Area Demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba9e56",
   "metadata": {},
   "source": [
    "This script takes three inputs:\n",
    "- the DAWA GeoJSON of Danish postal-code areas, containing the polygon boundaries for each `postal_code`  \n",
    "- a CSV of population statistics by postal code (data/population_df.csv), containing `neighborhood_code`, `neighborhood_name`, `postal_code`, `postal_area`, `postal_code_m2`, `Total`, `Men`, `Women`, `population_density_km2`\n",
    "- the interpolated pedestrian traffic CSV (data/expanded_foot_trafic.csv) from the previous step, containing `latitude`, `longitude`, `aadt_fod_7_19`, and `hvdt_fod_7_19`  \n",
    "\n",
    "It combines them into one clean output. First, it computes the geometric centroid of each postal-code polygon and merges these centroids with the population figures by matching on `postal_code`. Next for each point it checks which postal polygon contains it. Any points that fall outside all postal areas or lack full population data are removed. The result is a single CSV (data/pedestrian_with_zip_stats.csv) where each row gives a point’s latitude, longitude, interpolated pedestrian counts, its `postal_code`, and the linked demographic attributes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cf80709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8026/392023623.py:8: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  postal_gdf[\"zip_centroid\"] = postal_gdf.geometry.centroid\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Load postal areas and compute centroids\n",
    "postal_gdf = gpd.read_file(\"https://dawa.aws.dk/postnumre?format=geojson\").to_crs(\n",
    "    epsg=4326\n",
    ")\n",
    "postal_gdf[\"zip_centroid\"] = postal_gdf.geometry.centroid\n",
    "postal_gdf[\"zip_lon\"] = postal_gdf.zip_centroid.x\n",
    "postal_gdf[\"zip_lat\"] = postal_gdf.zip_centroid.y\n",
    "postal_gdf = postal_gdf.rename(columns={\"nr\": \"postal_code\"})[\n",
    "    [\"postal_code\", \"zip_lat\", \"zip_lon\", \"geometry\"]\n",
    "]\n",
    "\n",
    "# Load population data and merge\n",
    "pop = pd.read_csv(\"data/population_df.csv\", dtype={\"postal_code\": str})\n",
    "pop[\"postal_code\"] = pop[\"postal_code\"].str.zfill(4)\n",
    "postal_with_pop = postal_gdf.merge(pop, on=\"postal_code\", how=\"left\")\n",
    "\n",
    "# Load interpolated pedestrian traffic and convert to GeoDataFrame\n",
    "ped = pd.read_csv(\"data/expanded_foot_trafic.csv\")\n",
    "ped_gdf = gpd.GeoDataFrame(\n",
    "    ped, geometry=gpd.points_from_xy(ped.longitude, ped.latitude), crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Spatial join pedestrians to postal areas\n",
    "joined = gpd.sjoin(ped_gdf, postal_with_pop, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Remove rows where population stats are missing\n",
    "joined = joined.dropna(subset=[\"Total\", \"Men\", \"Women\", \"population_density_km2\"])\n",
    "\n",
    "# Select final columns and save\n",
    "joined[\n",
    "    [\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"aadt_fod_7_19\",\n",
    "        \"hvdt_fod_7_19\",\n",
    "        \"postal_code\",\n",
    "        \"Total\",\n",
    "        \"Men\",\n",
    "        \"Women\",\n",
    "        \"population_density_km2\",\n",
    "    ]\n",
    "].to_csv(\"data/pedestrian_with_zip_stats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa81a05",
   "metadata": {},
   "source": [
    "## 8th Step - Modifying End dates of restaurants\n",
    "As the info of the closing dates of the restaurants were available only for 2025, we decided to generate some mock data for having some closing dates also the years before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94210695",
   "metadata": {},
   "source": [
    "### First of all we need to clean the maps Dataset\n",
    "This code loads a dataset of Google Maps business reviews and performs several cleaning and transformation steps to prepare it for analysis. \n",
    "\n",
    "First, it standardizes the `Price Level` field by mapping price descriptions to numeric values and flags missing mappings.\n",
    "It then ensures that the `Rating` column is numeric and fills missing values with a default of 3.5. To improve accuracy,original ratings are remerged and used to restore missing or incorrect values. \n",
    "Similarly, the `Reviews` column is converted to numeric, with missing values filled with 5. The cleaned and enriched dataset is saved to a new CSV file named `mapsReviews_ds_NoNull.csv`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "mapsReviews_ds = pd.read_csv(\"data/maps_data_scraped.csv\")\n",
    "\n",
    "# Price level mapping\n",
    "price_mapping = {\n",
    "    \"1-100 kr a persona\": 1,\n",
    "    \"1-200 kr a persona\": 1,\n",
    "    \"100-200 kr a persona\": 1,\n",
    "    \"100-300 kr a persona\": 2,\n",
    "    \"200-300 kr a persona\": 2,\n",
    "    \"200-400 kr a persona\": 2,\n",
    "    \"300-400 kr a persona\": 3,\n",
    "    \"300-500 kr a persona\": 3,\n",
    "    \"400-500 kr a persona\": 3,\n",
    "    \"400-600 kr a persona\": 3,\n",
    "    \"500-600 kr a persona\": 3,\n",
    "    \"600-700 kr a persona\": 4,\n",
    "    \"Più di 1000 kr a persona\": 5,\n",
    "    \"2000-4000 kr a persona\": 5,\n",
    "    \"1-10 â\\x82¬ a persona\": 1,\n",
    "    \"30-40 â\\x82¬ a persona\": 2,\n",
    "}\n",
    "mapsReviews_ds[\"price_level_mapped\"] = (\n",
    "    mapsReviews_ds[\"Price Level\"].map(price_mapping).fillna(2)\n",
    ")\n",
    "mapsReviews_ds[\"price_level_missing\"] = (\n",
    "    mapsReviews_ds[\"Price Level\"].map(price_mapping).isna()\n",
    ")\n",
    "\n",
    "# Clean ratings\n",
    "mapsReviews_ds[\"Rating\"] = pd.to_numeric(\n",
    "    mapsReviews_ds[\"Rating\"], errors=\"coerce\"\n",
    ").fillna(3.5)\n",
    "mapsReviews_ds[\"rating_missing\"] = mapsReviews_ds[\"Rating\"].isna()\n",
    "\n",
    "# Restore from original ratings\n",
    "original_ratings = pd.read_csv(\"data/maps_data_scraped.csv\")[\n",
    "    [\"Input Name\", \"Rating\"]\n",
    "].dropna()\n",
    "mapsReviews_ds = mapsReviews_ds.merge(\n",
    "    original_ratings, on=\"Input Name\", how=\"left\", suffixes=(\"\", \"_original\")\n",
    ")\n",
    "\n",
    "mapsReviews_ds[\"Rating\"] = mapsReviews_ds[\"Rating_original\"].combine_first(\n",
    "    mapsReviews_ds[\"Rating\"]\n",
    ")\n",
    "mapsReviews_ds.drop(columns=[\"Rating_original\"], inplace=True)\n",
    "\n",
    "\n",
    "# Clean reviews\n",
    "mapsReviews_ds[\"Reviews\"] = pd.to_numeric(\n",
    "    mapsReviews_ds[\"Reviews\"], errors=\"coerce\"\n",
    ").fillna(5)\n",
    "\n",
    "# Save result\n",
    "mapsReviews_ds.to_csv(\"data/mapsReviews_ds_NoNull.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab722461",
   "metadata": {},
   "source": [
    "### Now we need to merge all the datasets into one\n",
    "This section prepares a merged dataset by fuzzy-matching restaurant names from two sources: a base dataset and a reviews dataset. \n",
    "\n",
    "It then enriches the data by attaching population statistics and estimating foot traffic. Postal codes are geocoded to obtain coordinates, which are used to find the nearest foot traffic measurement station. The result is a comprehensive dataset containing reviews, population density, and foot traffic for each restaurant. This will be used to calculate the updated End Date of restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d6223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 5761 (100.00%)\n",
      "Unmatched: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "from rapidfuzz import process\n",
    "from geopy.distance import geodesic\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "restaurants_ds = pd.read_csv(\"data/restaurants_df.csv\")\n",
    "population_ds = pd.read_csv(\"data/population_df.csv\")\n",
    "footTraffic_ds = pd.read_csv(\"data/expanded_foot_trafic.csv\")\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return str(text).lower().strip().replace(\"  \", \" \")\n",
    "\n",
    "\n",
    "# Puliamo nei due dataset\n",
    "restaurants_ds[\"Name_clean\"] = restaurants_ds[\"Name\"].apply(clean_text)\n",
    "restaurants_ds[\"Address_clean\"] = restaurants_ds[\"Address\"].apply(clean_text)\n",
    "\n",
    "mapsReviews_ds[\"Input Name_clean\"] = mapsReviews_ds[\"Input Name\"].apply(clean_text)\n",
    "mapsReviews_ds[\"Input Address_clean\"] = mapsReviews_ds[\"Input Address\"].apply(\n",
    "    clean_text\n",
    ")\n",
    "\n",
    "\n",
    "# Creiamo dizionario dei nomi delle review\n",
    "review_names = mapsReviews_ds[\"Input Name_clean\"].tolist()\n",
    "\n",
    "\n",
    "def fuzzy_match(name):\n",
    "    match = process.extractOne(\n",
    "        name, review_names, score_cutoff=80\n",
    "    )  # solo se >80% di similarità\n",
    "    if match:\n",
    "        return match[0]\n",
    "    return None\n",
    "\n",
    "\n",
    "# Applichiamo fuzzy matching\n",
    "restaurants_ds[\"matched_name\"] = restaurants_ds[\"Name_clean\"].apply(fuzzy_match)\n",
    "\n",
    "# Poi merge\n",
    "merged_df = restaurants_ds.merge(\n",
    "    mapsReviews_ds,\n",
    "    how=\"left\",\n",
    "    left_on=\"matched_name\",\n",
    "    right_on=\"Input Name_clean\",\n",
    "    suffixes=(\"\", \"_review\"),\n",
    ")\n",
    "\n",
    "\n",
    "# Quanti ristoranti hanno trovato Rating (cioè matchato?)\n",
    "matched = merged_df[\"Rating\"].notna().sum()\n",
    "\n",
    "# Quanti ristoranti NON hanno trovato Rating?\n",
    "unmatched = merged_df[\"Rating\"].isna().sum()\n",
    "\n",
    "# Quanti ristoranti totali?\n",
    "total = len(merged_df)\n",
    "\n",
    "# Percentuali\n",
    "matched_pct = (matched / total) * 100\n",
    "unmatched_pct = (unmatched / total) * 100\n",
    "\n",
    "# Stampa il risultato\n",
    "print(f\"Matched: {matched} ({matched_pct:.2f}%)\")\n",
    "print(f\"Unmatched: {unmatched} ({unmatched_pct:.2f}%)\")\n",
    "\n",
    "merged_df = merged_df.merge(\n",
    "    population_ds[\n",
    "        [\"postal_code\", \"Total\", \"population_density_km2\", \"restaurant_count\"]\n",
    "    ],\n",
    "    how=\"left\",\n",
    "    on=\"postal_code\",\n",
    "    suffixes=(\"\", \"_population\"),\n",
    ")\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"foot_traffic_matcher\")\n",
    "\n",
    "\n",
    "# Funzione che geocodifica un postal_code\n",
    "def get_coordinates(postal_code):\n",
    "    try:\n",
    "        location = geolocator.geocode(f\"{postal_code} Copenhagen Denmark\")\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# Creiamo una nuova tabella con CAP unici\n",
    "postal_code_unique = merged_df[\"postal_code\"].dropna().unique()\n",
    "\n",
    "# Geocodifica di tutti i CAP\n",
    "postal_code_coords = []\n",
    "for code in postal_code_unique:\n",
    "    lat, lon = get_coordinates(code)\n",
    "    postal_code_coords.append((code, lat, lon))\n",
    "    time.sleep(1)  # importantissimo per non farsi bloccare\n",
    "\n",
    "postal_coords_df = pd.DataFrame(\n",
    "    postal_code_coords, columns=[\"postal_code\", \"latitude\", \"longitude\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Funzione per trovare il foot traffic della stazione più vicina\n",
    "def find_nearest_traffic(lat, lon, stations_df):\n",
    "    distances = stations_df.apply(\n",
    "        lambda row: geodesic((lat, lon), (row[\"latitude\"], row[\"longitude\"])).meters,\n",
    "        axis=1,\n",
    "    )\n",
    "    nearest_idx = distances.idxmin()\n",
    "    return stations_df.loc[\n",
    "        nearest_idx, \"aadt_fod_7_19\"\n",
    "    ]  # puoi cambiare con 'hvdt_fod_7_19' se vuoi picco\n",
    "\n",
    "\n",
    "# Applichiamo a tutti i CAP\n",
    "postal_coords_df[\"foot_traffic\"] = postal_coords_df.apply(\n",
    "    lambda row: find_nearest_traffic(row[\"latitude\"], row[\"longitude\"], footTraffic_ds),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Merge su postal_code\n",
    "merged_df = merged_df.merge(\n",
    "    postal_coords_df[[\"postal_code\", \"latitude\", \"longitude\", \"foot_traffic\"]],\n",
    "    how=\"left\",\n",
    "    on=\"postal_code\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9bb83",
   "metadata": {},
   "source": [
    "### With the final merged dataset we can compute the new closing Date\n",
    "\n",
    "In this part of the project, we simulate the expected closure dates of businesses based on factors such as rating, reviews, price level, foot traffic, and population density. A risk score is computed for each business and used to probabilistically determine how many years it remains open. Businesses predicted to close have their simulated end dates generated accordingly. The dataset is then updated to reflect closure status, duration of activity, and cleaned of inconsistencies.\n",
    "\n",
    "Following the simulation, we prepare the final dataset for export by selecting relevant columns, applying fuzzy matching to merge business metadata from an external source, and renaming fields for clarity. The result is a structured dataset ready for analysis, containing both observed and simulated information about business performance and survival.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd004c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Function to simulate closure time and compute risk based on business features\n",
    "def simulate_closure_date(row):\n",
    "    # Parse rating safely\n",
    "    try:\n",
    "        rating = float(row[\"Rating\"])\n",
    "    except:\n",
    "        rating = 3.5\n",
    "\n",
    "    risk = 0\n",
    "\n",
    "    # Rating contribution\n",
    "    if rating < 3.5:\n",
    "        risk += 0.4\n",
    "    elif rating > 4.2:\n",
    "        risk += 0.1\n",
    "    else:\n",
    "        risk += 0.25\n",
    "\n",
    "    # Extreme price levels\n",
    "    if row[\"price_level_mapped\"] in [1, 5]:\n",
    "        risk += 0.15\n",
    "\n",
    "    # Low review count\n",
    "    try:\n",
    "        reviews = float(row[\"Reviews\"])\n",
    "    except:\n",
    "        reviews = 5\n",
    "    if reviews < 10:\n",
    "        risk += 0.1\n",
    "\n",
    "    # Low foot traffic\n",
    "    if pd.notna(row.get(\"foot_traffic\")) and float(row[\"foot_traffic\"]) < 3000:\n",
    "        risk += 0.15\n",
    "\n",
    "    # Low population density\n",
    "    if (\n",
    "        pd.notna(row.get(\"population_density_km2\"))\n",
    "        and float(row[\"population_density_km2\"]) < 5000\n",
    "    ):\n",
    "        risk += 0.1\n",
    "\n",
    "    risk = min(risk, 1.0)\n",
    "\n",
    "    # Use risk score to influence closure time simulation\n",
    "    r = np.random.rand()\n",
    "    if r < 0.6 + 0.3 * risk:\n",
    "        closure_years = np.random.randint(2, 8)\n",
    "    elif r < 0.9 + 0.08 * (1 - risk):\n",
    "        closure_years = np.random.randint(8, 16)\n",
    "    else:\n",
    "        closure_years = min(np.random.geometric(0.02), 30)\n",
    "\n",
    "    return pd.Series({\"closure_years\": closure_years, \"closure_risk\": risk})\n",
    "\n",
    "\n",
    "# Apply the closure time and risk computation\n",
    "merged_df[[\"closure_years\", \"closure_risk\"]] = merged_df.apply(\n",
    "    simulate_closure_date, axis=1\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate a random value only for active businesses\n",
    "mask_open = merged_df[\"active\"] == True\n",
    "merged_df.loc[mask_open, \"random_val\"] = np.random.rand(mask_open.sum())\n",
    "\n",
    "# Determine which active businesses will close\n",
    "merged_df[\"closure_simulated\"] = False\n",
    "merged_df.loc[mask_open, \"closure_simulated\"] = (\n",
    "    merged_df.loc[mask_open, \"random_val\"] < merged_df.loc[mask_open, \"closure_risk\"]\n",
    ")\n",
    "\n",
    "# Convert Startdate to datetime safely\n",
    "merged_df[\"Startdate\"] = pd.to_datetime(merged_df[\"Startdate\"], errors=\"coerce\")\n",
    "\n",
    "# Today's date\n",
    "today = pd.to_datetime(\"today\")\n",
    "\n",
    "# Simulate end dates for newly closed businesses\n",
    "mask_newly_closed = mask_open & merged_df[\"closure_simulated\"]\n",
    "merged_df.loc[mask_newly_closed, \"Simulated_EndDate\"] = merged_df.loc[\n",
    "    mask_newly_closed, \"Startdate\"\n",
    "] + pd.to_timedelta(\n",
    "    np.random.randint(2 * 365, 7 * 365, size=mask_newly_closed.sum()), unit=\"D\"\n",
    ")\n",
    "\n",
    "# Mark businesses as inactive if they closed\n",
    "merged_df.loc[mask_newly_closed, \"active\"] = False\n",
    "\n",
    "# Ensure Simulated_EndDate is in datetime format\n",
    "merged_df[\"Simulated_EndDate\"] = pd.to_datetime(merged_df[\"Simulated_EndDate\"])\n",
    "\n",
    "# Compute survival duration in days\n",
    "merged_df[\"duration_days\"] = (\n",
    "    merged_df[\"Simulated_EndDate\"].fillna(today) - merged_df[\"Startdate\"]\n",
    ").dt.days\n",
    "\n",
    "# Save the final dataset\n",
    "# merged_df.to_csv(\"merged_restaurants_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d195c322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sicurezza: Simulated_EndDate è datetime\n",
    "merged_df[\"Simulated_EndDate\"] = pd.to_datetime(\n",
    "    merged_df[\"Simulated_EndDate\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Assicuriamoci Startdate sia datetime\n",
    "merged_df[\"Startdate\"] = pd.to_datetime(merged_df[\"Startdate\"], errors=\"coerce\")\n",
    "\n",
    "# Se la chiusura è futura ➔ resettiamo\n",
    "mask_future_closures = merged_df[\"Simulated_EndDate\"] > today\n",
    "\n",
    "# Mettiamo NaT dove la data è futura\n",
    "merged_df.loc[mask_future_closures, \"Simulated_EndDate\"] = pd.NaT\n",
    "\n",
    "# Aggiorniamo anche is_closed\n",
    "merged_df[\"is_closed\"] = merged_df[\"Simulated_EndDate\"].notna().astype(int)\n",
    "\n",
    "# Ricalcoliamo duration_days\n",
    "merged_df[\"duration_days\"] = (\n",
    "    merged_df[\"Simulated_EndDate\"].fillna(today) - merged_df[\"Startdate\"]\n",
    ").dt.days\n",
    "\n",
    "# (Opzionale) duration in anni\n",
    "merged_df[\"duration_years\"] = merged_df[\"duration_days\"] / 365\n",
    "\n",
    "merged_df = merged_df.drop_duplicates(\"Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "550b6860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches found: 4224/4224 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rapidfuzz import process\n",
    "\n",
    "# Load the scraped dataset\n",
    "scraped_df = pd.read_csv(\"data/scraped_companies_combined_clean_with_coords.csv\")\n",
    "\n",
    "# Remove duplicate companies based on the name\n",
    "scraped_df = scraped_df.drop_duplicates(\"name\")\n",
    "\n",
    "# Columns to keep from the merged dataset\n",
    "columns_to_keep = [\n",
    "    \"Name\",\n",
    "    \"Address\",\n",
    "    \"Startdate\",\n",
    "    \"Simulated_EndDate\",\n",
    "    \"code\",\n",
    "    \"postal_code\",\n",
    "    \"Rating\",\n",
    "    \"Reviews\",\n",
    "    \"price_level_mapped\",\n",
    "    \"duration_days\",\n",
    "]\n",
    "\n",
    "# Select and filter only the desired columns\n",
    "filtered_df = merged_df[columns_to_keep].copy()\n",
    "\n",
    "# Clean names and addresses in both datasets\n",
    "filtered_df[\"Name_clean\"] = filtered_df[\"Name\"].apply(clean_text)\n",
    "filtered_df[\"Address_clean\"] = filtered_df[\"Address\"].apply(clean_text)\n",
    "scraped_df[\"name_clean\"] = scraped_df[\"name\"].apply(clean_text)\n",
    "scraped_df[\"address_clean\"] = scraped_df[\"address\"].apply(clean_text)\n",
    "\n",
    "# Drop irrelevant column if present\n",
    "scraped_df = scraped_df.drop(columns=[\"company type\"], errors=\"ignore\")\n",
    "\n",
    "# Create matching keys for fuzzy matching\n",
    "filtered_df[\"match_key\"] = (\n",
    "    filtered_df[\"Name_clean\"] + \" \" + filtered_df[\"Address_clean\"]\n",
    ")\n",
    "scraped_df[\"match_key\"] = scraped_df[\"name_clean\"] + \" \" + scraped_df[\"address_clean\"]\n",
    "\n",
    "# Create a list of keys from the scraped dataset\n",
    "scraped_keys = scraped_df[\"match_key\"].tolist()\n",
    "\n",
    "\n",
    "# Fuzzy matching function\n",
    "def fuzzy_match(key):\n",
    "    match = process.extractOne(key, scraped_keys, score_cutoff=85)\n",
    "    return match[0] if match else None\n",
    "\n",
    "\n",
    "# Apply fuzzy matching to each restaurant\n",
    "filtered_df[\"matched_key\"] = filtered_df[\"match_key\"].apply(fuzzy_match)\n",
    "\n",
    "# Report number of matches found\n",
    "matched = filtered_df[\"matched_key\"].notna().sum()\n",
    "total = len(filtered_df)\n",
    "print(f\"Matches found: {matched}/{total} ({matched / total:.1%})\")\n",
    "\n",
    "# Merge datasets on matched keys\n",
    "merged_df = filtered_df.merge(\n",
    "    scraped_df,\n",
    "    how=\"left\",\n",
    "    left_on=\"matched_key\",\n",
    "    right_on=\"match_key\",\n",
    "    suffixes=(\"\", \"_scraped\"),\n",
    ")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\n",
    "    \"name\",\n",
    "    \"address\",\n",
    "    \"p_nummer\",\n",
    "    \"startdate\",\n",
    "    \"enddate\",\n",
    "    \"branchekode\",\n",
    "    \"zip\",\n",
    "    \"active\",\n",
    "    \"match_key_scraped\",\n",
    "    \"Name_clean\",\n",
    "    \"Address_clean\",\n",
    "    \"matched_key\",\n",
    "    \"name_clean\",\n",
    "    \"address_clean\",\n",
    "    \"match_key\",\n",
    "]\n",
    "mergedFinal_df = merged_df.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "\n",
    "# Rename columns for clarity\n",
    "mergedFinal_df = mergedFinal_df.rename(\n",
    "    columns={\n",
    "        \"Name\": \"Restaurant_Name\",\n",
    "        \"Address\": \"Restaurant_Address\",\n",
    "        \"Startdate\": \"Opening_Date\",\n",
    "        \"Simulated_EndDate\": \"Closing_Date\",\n",
    "        \"code\": \"Branchekod\",\n",
    "        \"postal_code\": \"Postal_Code\",\n",
    "        \"Rating\": \"Rating\",\n",
    "        \"Reviews\": \"Number_of_Reviews\",\n",
    "        \"price_level_mapped\": \"Price_Level\",\n",
    "        \"duration_days\": \"Duration_in_Days\",\n",
    "        \"status\": \"Status\",\n",
    "        \"latitude\": \"Latitude\",\n",
    "        \"longitude\": \"Longitude\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Remove rows with missing opening date\n",
    "mergedFinal_df = mergedFinal_df.dropna(subset=[\"Opening_Date\"])\n",
    "\n",
    "# Save final dataset to CSV\n",
    "mergedFinal_df.to_csv(\"data/mergedFinal_ds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3717fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two datasets using the fuzzy-matched key\n",
    "merged_df = filtered_df.merge(\n",
    "    scraped_df,\n",
    "    how=\"left\",\n",
    "    left_on=\"matched_key\",\n",
    "    right_on=\"match_key\",\n",
    "    suffixes=(\"\", \"_scraped\"),\n",
    ")\n",
    "\n",
    "# Columns to drop after merge (unneeded or redundant)\n",
    "columns_to_drop = [\n",
    "    \"name\",\n",
    "    \"address\",\n",
    "    \"p_nummer\",\n",
    "    \"startdate\",\n",
    "    \"enddate\",\n",
    "    \"branchekode\",\n",
    "    \"zip\",\n",
    "    \"active\",\n",
    "    \"match_key_scraped\",\n",
    "    \"Name_clean\",\n",
    "    \"Address_clean\",\n",
    "    \"matched_key\",\n",
    "    \"name_clean\",\n",
    "    \"address_clean\",\n",
    "    \"match_key\",\n",
    "]\n",
    "\n",
    "# Drop columns safely, ignore if not present\n",
    "mergedFinal_df = merged_df.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "\n",
    "# Rename columns for clarity and consistency\n",
    "mergedFinal_df = mergedFinal_df.rename(\n",
    "    columns={\n",
    "        \"Name\": \"Restaurant_Name\",\n",
    "        \"Address\": \"Restaurant_Address\",\n",
    "        \"Startdate\": \"Opening_Date\",\n",
    "        \"Simulated_EndDate\": \"Closing_Date\",\n",
    "        \"code\": \"Branchekod\",\n",
    "        \"postal_code\": \"Postal_Code\",\n",
    "        \"Rating\": \"Rating\",\n",
    "        \"Reviews\": \"Number_of_Reviews\",\n",
    "        \"price_level_mapped\": \"Price_Level\",\n",
    "        \"duration_days\": \"Duration_in_Days\",\n",
    "        \"status\": \"Status\",\n",
    "        \"latitude\": \"Latitude\",\n",
    "        \"longitude\": \"Longitude\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Remove rows missing an opening date\n",
    "mergedFinal_df = mergedFinal_df.dropna(subset=[\"Opening_Date\"])\n",
    "\n",
    "# Save the cleaned and formatted DataFrame to CSV\n",
    "mergedFinal_df.to_csv(\"data/mergedFinal_ds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd149e1",
   "metadata": {},
   "source": [
    "## 9th Step - Creating the Final Dataset for Survival Analysis by adding restaurant information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0e6e0b",
   "metadata": {},
   "source": [
    "This script creates the final dataset used to perform the survival analysis. To build this dataset, two sources are merged:\n",
    "\n",
    "- The dataset generated at step 7 (data/pedestrian_with_zip_stats.csv), which contains a 200-meter grid covering the Copenhagen municipality. Each point includes interpolated pedestrian traffic values and demographic data such as population density and gender breakdown.\n",
    "- A dataset of restaurants (mergedFinal_ds.csv) that includes the following fields:\n",
    "  - `Restaurant_Name`, `Restaurant_Address`: identification and location details  \n",
    "  - `Opening_Date`, `Closing_Date`: used to calculate how long each restaurant has been in operation  \n",
    "  - `Branchekod`: an industry code classifying the type of restaurant  \n",
    "  - `Postal_Code`: used for linking with demographic data  \n",
    "  - `Rating`, `Number_of_Reviews`, `Price_Level`: customer review and pricing information  \n",
    "  - `Latitude`, `Longitude`: geographic coordinates of the restaurant  \n",
    "\n",
    "The goal of the script is to assign to each coordinate in the grid the aggregated characteristics of restaurants located within a 200-meter radius. First, a circular buffer is created around each grid point, and restaurants that fall within these buffers are identified using a spatial join. For each grid point, the script computes several statistics: the total number of restaurants nearby, how many are still open or have closed, their average and median survival times (in days, months, and years), the number of openings and closures in the last three years, and the mean values of customer ratings, number of reviews, and price levels.\n",
    "\n",
    "Additionally, the frequency of each restaurant type (based on `Branchekod`) is computed and added as separate columns. These aggregated values are merged back into the original grid, which is then reprojected to latitude and longitude.\n",
    "\n",
    "The result is saved as (data/df_survival_analysis.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f59ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8026/1748939963.py:15: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  rest = pd.read_csv(\n",
      "/tmp/ipykernel_8026/1748939963.py:15: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  rest = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved df_survival_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from datetime import datetime\n",
    "\n",
    "radius_m = 200\n",
    "\n",
    "# Load grid with zip/pop and convert to metric CRS\n",
    "grid = pd.read_csv(\"data/pedestrian_with_zip_stats.csv\")\n",
    "grid_gdf = gpd.GeoDataFrame(\n",
    "    grid, geometry=gpd.points_from_xy(grid.longitude, grid.latitude), crs=\"EPSG:4326\"\n",
    ").to_crs(epsg=3857)\n",
    "\n",
    "# Load and prepare restaurant data\n",
    "rest = pd.read_csv(\n",
    "    \"data/mergedFinal_ds.csv\",\n",
    "    parse_dates=[\"Opening_Date\", \"Closing_Date\"],\n",
    "    dayfirst=True,\n",
    ").rename(\n",
    "    columns={\n",
    "        \"Opening_Date\": \"startdate\",\n",
    "        \"Closing_Date\": \"enddate\",\n",
    "        \"Price_Level\": \"price_level\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Clean and convert numeric columns\n",
    "for col in [\"Rating\", \"Number_of_Reviews\", \"price_level\"]:\n",
    "    rest[col] = (\n",
    "        rest[col]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "        .replace(\"nan\", pd.NA)\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "    \n",
    "rest_gdf = gpd.GeoDataFrame(\n",
    "    rest, geometry=gpd.points_from_xy(rest.Longitude, rest.Latitude), crs=\"EPSG:4326\"\n",
    ").to_crs(epsg=3857)\n",
    "\n",
    "\n",
    "\n",
    "# Compute survival time\n",
    "today = pd.Timestamp(datetime.today())\n",
    "rest_gdf[\"survival_days\"] = (\n",
    "    rest_gdf[\"enddate\"].fillna(today) - rest_gdf[\"startdate\"]\n",
    ").dt.days\n",
    "\n",
    "# Buffer grid points and spatially join restaurants\n",
    "buffers = grid_gdf.copy()\n",
    "buffers[\"geometry\"] = buffers.geometry.buffer(radius_m)\n",
    "joined = gpd.sjoin(\n",
    "    rest_gdf, buffers[[\"geometry\"]], how=\"inner\", predicate=\"within\"\n",
    ").rename(columns={\"index_right\": \"grid_index\"})\n",
    "\n",
    "\n",
    "# Aggregate restaurant stats per grid cell\n",
    "def agg_stats(df):\n",
    "    now3 = today - pd.DateOffset(years=3)\n",
    "    sd = df[\"survival_days\"]\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"num_restaurants_total\": len(df),\n",
    "            \"num_restaurants_open\": df[\"enddate\"].isna().sum(),\n",
    "            \"num_restaurants_closed\": df[\"enddate\"].notna().sum(),\n",
    "            \"avg_survival_days\": sd.mean(),\n",
    "            \"median_survival_days\": sd.median(),\n",
    "            \"avg_survival_months\": sd.mean() / 30.44,\n",
    "            \"avg_survival_years\": sd.mean() / 365.25,\n",
    "            \"openings_last_3_years\": (df[\"startdate\"] >= now3).sum(),\n",
    "            \"closures_last_3_years\": (df[\"enddate\"] >= now3).sum(),\n",
    "            \"mean_rating\": df[\"Rating\"].mean(),\n",
    "            \"mean_reviews\": df[\"Number_of_Reviews\"].mean(),\n",
    "            \"mean_price_level\": df[\"price_level\"].mean(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "stats = joined.groupby(\"grid_index\").apply(agg_stats)\n",
    "\n",
    "# Pivot branch codes and join\n",
    "code_counts = (\n",
    "    joined.groupby([\"grid_index\", \"Branchekod\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .add_prefix(\"code_\")\n",
    ")\n",
    "stats = stats.join(code_counts, how=\"left\").fillna(0)\n",
    "\n",
    "# Merge back onto grid, reproject, extract coords\n",
    "result = grid_gdf.join(stats, how=\"left\").fillna(0).to_crs(epsg=4326)\n",
    "result[\"latitude\"] = result.geometry.y\n",
    "result[\"longitude\"] = result.geometry.x\n",
    "\n",
    "# Select final columns and save\n",
    "base_cols = [\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"aadt_fod_7_19\",\n",
    "    \"hvdt_fod_7_19\",\n",
    "    \"postal_code\",\n",
    "    \"Total\",\n",
    "    \"Men\",\n",
    "    \"Women\",\n",
    "    \"population_density_km2\",\n",
    "    \"num_restaurants_total\",\n",
    "    \"num_restaurants_open\",\n",
    "    \"num_restaurants_closed\",\n",
    "    \"avg_survival_days\",\n",
    "    \"median_survival_days\",\n",
    "    \"avg_survival_months\",\n",
    "    \"avg_survival_years\",\n",
    "    \"openings_last_3_years\",\n",
    "    \"closures_last_3_years\",\n",
    "    \"mean_rating\",\n",
    "    \"mean_reviews\",\n",
    "    \"mean_price_level\",\n",
    "]\n",
    "code_cols = [c for c in stats.columns if c.startswith(\"code_\")]\n",
    "final_df = result[base_cols + code_cols]\n",
    "\n",
    "final_df.to_csv(\"data/df_survival_analysis.csv\", index=False)\n",
    "print(\"Saved df_survival_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc86077d",
   "metadata": {},
   "source": [
    "The table below describes the columns included in the final dataset `df_survival_analysis.csv` where each row represents a 200-meter grid point and includes pedestrian traffic, demographic statistics, and aggregated information about nearby restaurants.  \n",
    "\n",
    "\n",
    "| Column                     | Description                                                                                       |\n",
    "|----------------------------|---------------------------------------------------------------------------------------------------|\n",
    "| **latitude**               | Grid point latitude (decimal degrees, WGS84)                                                      |\n",
    "| **longitude**              | Grid point longitude (decimal degrees, WGS84)                                                     |\n",
    "| **aadt_fod_7_19**          | Interpolated average daily pedestrian volume (between 07:00–19:00)                                 |\n",
    "| **hvdt_fod_7_19**          | Interpolated peak pedestrian volume (between 07:00–19:00)                                          |\n",
    "| **postal_code**            | Danish postal code containing the grid point                                                      |\n",
    "| **Total**                  | Total population in the postal area                                                               |\n",
    "| **Men**                    | Male population in the postal area                                                                |\n",
    "| **Women**                  | Female population in the postal area                                                              |\n",
    "| **population_density_km2** | Population density (people per km²) in the postal area                                            |\n",
    "| **num_restaurants_total**  | Total number of restaurants within 200 m of the grid point                                        |\n",
    "| **num_restaurants_open**   | Number of those restaurants still open                                                            |\n",
    "| **num_restaurants_closed** | Number of those restaurants that have closed                                                      |\n",
    "| **avg_survival_days**      | Mean restaurant “survival” time (days between opening and closing or today)                       |\n",
    "| **median_survival_days**   | Median restaurant “survival” time (days)                                                          |\n",
    "| **avg_survival_months**    | Mean survival time converted to months (days/30.44)                                               |\n",
    "| **avg_survival_years**     | Mean survival time converted to years (days/365.25)                                               |\n",
    "| **openings_last_3_years**  | Count of restaurant openings in the past 3 years                                                  |\n",
    "| **closures_last_3_years**  | Count of restaurant closures in the past 3 years                                                  |\n",
    "| **mean_rating**            | Average customer rating of restaurants in the buffer                                              |\n",
    "| **mean_reviews**           | Average number of reviews per restaurant in the buffer                                            |\n",
    "| **mean_price_level**       | Average price-level indicator of restaurants in the buffer                                        |\n",
    "| **code_<Branchekod>**      | Number of restaurants with branch code `<Branchekod>` within 200 m (one column per branch code)   |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course02509",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
