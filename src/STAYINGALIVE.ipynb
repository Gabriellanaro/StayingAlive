{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8681ab4e",
   "metadata": {},
   "source": [
    "# Stayin' Alive\n",
    "### *An AI-Powered Tool for Optimal Restaurant and Bar Location Selection and Business Longevity*\n",
    "42578 – Advanced Business Analytics, DTU, 2025 <br>\n",
    "Group 21 - Crocs Validation<br>\n",
    "Giulia Andreatta -sxxxx<br>\n",
    "Gabriel Lanaro - sxxxx<br>\n",
    "Alessia Saccardo - sxxxx<br>\n",
    "Gabriele Turetta - sxxxx<br>\n",
    "\n",
    "### Objective\n",
    "Opening a restaurant or a bar is a high-risk endeavor—many establishments close within their first few years. In Copenhagen, aspiring restaurateurs and investors often lack a data-driven approach when selecting a location. Moreover, understanding the reasons behind a restaurant’s success or failure remains a challenge.\n",
    "\n",
    "This project aims to:\n",
    "\n",
    "- Recommend optimal locations for new restaurants or bars using Survival Analysis.\n",
    "- Visualize location suitability through an interactive heatmap enriched with predictive longevity scores, pedestrian peak hours, density of restaurants, pins of active and closed activities.\n",
    "\n",
    "### Datasets\n",
    "\n",
    "- **Company data scraped from the official CVR registry via [virk.dk](https://datacvr.virk.dk/soegeresultater?fritekst=d&sideIndex=0&size=10)**<br>\n",
    "Includes business registration details, location, restaurant closures, branchekode\n",
    "\n",
    "- **Google Maps Scraped Data**<br>\n",
    "Includes business location, rating, number of reviews, price range, tags.\n",
    "\n",
    "- **Pedestrian Dataset from [OpenData.dk](https://www.opendata.dk/city-of-copenhagen/taelling_fodg#:~:text=Number%20of%20pedestrians%20counted%20on,19%20in%20both%20directions)**<br>\n",
    "Provides foot traffic counts recorded at specific times and locations in Copenhagen\n",
    "\n",
    "### ABA Topics Covered\n",
    "- **Web Data Mining**\n",
    "Scraping large-scale data from Google Maps and government databases to construct the datasets.\n",
    "\n",
    "- **Survival Analysis**\n",
    "Predicting restaurant longevity using Kaplan-Meier and Cox Proportional Hazards models.\n",
    "\n",
    "- **Recommender Systems**\n",
    "Suggesting location options for new restaurants and bars based on market gaps and existing competition.\n",
    "\n",
    "- **AI in the Real World**\n",
    "Delivering real value to stakeholders by supporting data-driven restaurant planning and resilience strategies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bab528",
   "metadata": {},
   "source": [
    "## 1st Step - Data Scraping from the official CVR registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41cae2e",
   "metadata": {},
   "source": [
    "This script performs web scraping on the Danish company registry website (https://datacvr.virk.dk)\n",
    "to extract company details for active business units in specified industry sectors (branchekoder).\n",
    "It uses Selenium to navigate the search results, extract key information for each business unit,\n",
    "and follow links to detailed company pages to obtain start and end dates.\n",
    "\n",
    "The results are saved in a CSV file, and duplicate entries (based on P-number) are avoided by \n",
    "keeping track of already seen values. The script is designed to be resumed without duplicating \n",
    "previous entries.\n",
    "\n",
    "Required: chromedriver installed and path correctly set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "driver_path = r\"C:\\\\programmi\\\\chromedriver\\\\chromedriver.exe\"\n",
    "options = Options()\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(driver_path), options=options)\n",
    "\n",
    "'''\n",
    "the loop can be run for each branchekode separately, or all at once by uncommenting the lines below.\n",
    "In our project, the branchekodes were manually uncommented, to create a single csv file for each branchekode.\n",
    "This was done to avoid the script from running for too long, risking on IP bans or website crashes.\n",
    "The single csv files were then merged into a single csv file.\n",
    "'''\n",
    "branchekodes = [\n",
    "    # 561110,   # serving food in restaurants and cafes\n",
    "    # 561190,   # includes the operation of restaurants, where the main emphasis is on takeaway with very limited table service.\n",
    "    # 563010,   # includes serving beverages, possibly with some edibles, but where the main emphasis is on serving non-alcoholic beverages for immediate consumption on site.\n",
    "    563020,     #  includes serving beverages, possibly with some edibles, but where the main emphasis is on serving alcoholic beverages for immediate consumption on the premises.\n",
    "]\n",
    "\n",
    "for branchekode in branchekodes:\n",
    "    page = 0\n",
    "    csv_file_path = f\"scraped_companies_{branchekode}_active.csv\"\n",
    "    header = [\"Name\", \"Address\", \"P-nummer\", \"Status\", \"Company Type\", \"Startdate\", \"Enddate\"]\n",
    "    pnummer_seen = set()\n",
    "\n",
    "    # If file exists, read already saved P-numbers\n",
    "    file_exists = os.path.exists(csv_file_path)\n",
    "    if file_exists:\n",
    "        with open(csv_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                pnummer_seen.add(row[\"P-nummer\"])\n",
    "    else:\n",
    "        # Create file and write header\n",
    "        with open(csv_file_path, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=header)\n",
    "            writer.writeheader()\n",
    "\n",
    "    while True:\n",
    "        url = f\"https://datacvr.virk.dk/soegeresultater?sideIndex={page}&enhedstype=produktionsenhed&region=29190623&branchekode={branchekode}\"\n",
    "        print(f\"Scraping page {page}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        rows = driver.find_elements(By.CSS_SELECTOR, 'div[data-cy=\"soegeresultater-tabel\"] > div.row')\n",
    "\n",
    "        if not rows:\n",
    "            print(\"No data found. Stopping.\")\n",
    "            break\n",
    "\n",
    "        for row in rows:\n",
    "            try:\n",
    "                name = row.find_element(By.CSS_SELECTOR, \"span.bold.value\").text.strip()\n",
    "\n",
    "                address_block = row.find_element(By.CSS_SELECTOR, \"div.col-12.col-lg-4\")\n",
    "                address_lines = address_block.text.strip().split(\"\\n\")[-2:]\n",
    "                address = \", \".join(address_lines)\n",
    "\n",
    "                pnummer = row.find_element(By.XPATH, './/div[div[text()=\"P-nummer:\"]]/div[2]').text.strip()\n",
    "\n",
    "                # Skip if already saved\n",
    "                if pnummer in pnummer_seen:\n",
    "                    continue\n",
    "                pnummer_seen.add(pnummer)\n",
    "\n",
    "                status = row.find_element(By.XPATH, './/div[div[text()=\"Status:\"]]/div[2]').text.strip()\n",
    "                form = row.find_element(By.XPATH, './/div[div[text()=\"Virksomhedsform:\"]]/div[2]').text.strip()\n",
    "\n",
    "                link_elem = row.find_element(By.CSS_SELECTOR, 'div[data-cy=\"vis-mere\"] a')\n",
    "                link = link_elem.get_attribute(\"href\")\n",
    "\n",
    "                # Open detail page in new tab\n",
    "                driver.execute_script(\"window.open('');\")\n",
    "                driver.switch_to.window(driver.window_handles[1])\n",
    "                driver.get(link)\n",
    "                time.sleep(3)\n",
    "\n",
    "                # Extract dates\n",
    "                startdato = \"\"\n",
    "                ophoersdato = \"\"\n",
    "\n",
    "                try:\n",
    "                    startdato_element = driver.find_element(\n",
    "                        By.XPATH, '//div[(strong[text()=\"Startdato\"] or span[text()=\"Startdato\"])]/following-sibling::div'\n",
    "                    )\n",
    "                    startdato = startdato_element.text.strip()\n",
    "                except:\n",
    "                    startdato = \"\"\n",
    "\n",
    "                try:\n",
    "                    ophoersdato_element = driver.find_element(\n",
    "                        By.XPATH, '//div[(strong[text()=\"Ophørsdato\"] or span[text()=\"Ophørsdato\"])]/following-sibling::div'\n",
    "                    )\n",
    "                    ophoersdato = ophoersdato_element.text.strip()\n",
    "                except:\n",
    "                    ophoersdato = \"\"\n",
    "\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "                # Write to CSV\n",
    "                with open(csv_file_path, \"a\", newline='', encoding=\"utf-8\") as f:\n",
    "                    writer = csv.DictWriter(f, fieldnames=header)\n",
    "                    writer.writerow({\n",
    "                        \"Name\": name,\n",
    "                        \"Address\": address,\n",
    "                        \"P-nummer\": pnummer,\n",
    "                        \"Status\": status,\n",
    "                        \"Company Type\": form,\n",
    "                        \"Startdate\": startdato,\n",
    "                        \"Enddate\": ophoersdato\n",
    "                    })\n",
    "\n",
    "                print(f\"{name} | {startdato} → {ophoersdato}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Error during parsing:\", e)\n",
    "                continue\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "driver.quit()\n",
    "print(\"Scraping finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81ae89",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stayingalive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
